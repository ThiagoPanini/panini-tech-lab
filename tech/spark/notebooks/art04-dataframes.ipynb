{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|numero|\n",
      "+------+\n",
      "|     0|\n",
      "|     1|\n",
      "|     2|\n",
      "|     3|\n",
      "|     4|\n",
      "+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Inicializando sessão\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"art04-dataframes\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Criando um DataFrame básico\n",
    "df_range = spark.range(1000).toDF(\"numero\")\n",
    "df_range.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizando tipo primitivo do objeto\n",
    "type(df_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O DataFrame criado possui 4 partições\n"
     ]
    }
   ],
   "source": [
    "# Verificando quantidade de partições do DataFrame\n",
    "num_partitions = df_range.rdd.getNumPartitions()\n",
    "print(f'O DataFrame criado possui {num_partitions} partições')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numero: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificando schema do DataFrame\n",
    "df_range.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Inicializando sessão\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"art04-dataframes\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Definindo variáveis de leitura de dados\n",
    "parent_dir = ''.join(os.path.pardir + \"/\") * 3\n",
    "file_path = 'data/flights-data/summary-data/csv/2015-summary.csv'\n",
    "data_path = os.path.join(parent_dir, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando a leitura de arquivo csv\n",
    "df_csv = (spark.read.format(\"csv\")\n",
    "          .option(\"inferSchema\", \"true\")\n",
    "          .option(\"header\", \"true\")\n",
    "          .load(data_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizando algumas linhas do DataFrame\n",
    "df_csv.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizando schema dos dados\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- pais_destino: string (nullable = true)\n",
      " |-- pais_origem: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "None\n",
      "+-------------+-------------+-----+\n",
      "| pais_destino|  pais_origem|count|\n",
      "+-------------+-------------+-----+\n",
      "|United States|      Romania|   15|\n",
      "|United States|      Croatia|    1|\n",
      "|United States|      Ireland|  344|\n",
      "|        Egypt|United States|   15|\n",
      "|United States|        India|   62|\n",
      "|United States|    Singapore|    1|\n",
      "|United States|      Grenada|   62|\n",
      "|   Costa Rica|United States|  588|\n",
      "|      Senegal|United States|   40|\n",
      "|      Moldova|United States|    1|\n",
      "+-------------+-------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando tipos primitivos a serem utilizados\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Definindo schema\n",
    "flights_schema = StructType([\n",
    "    StructField(\"pais_destino\", StringType(), nullable=True),\n",
    "    StructField(\"pais_origem\", StringType(), nullable=True),\n",
    "    StructField(\"count\", IntegerType(), nullable=False,\n",
    "        metadata={\"desc\": \"quantidade de viagens feitas da origem para o destino\"})\n",
    "])\n",
    "\n",
    "# Lendo dados com schema definido\n",
    "df_csv_schema = (spark.read.format(\"csv\")\n",
    "    .schema(flights_schema)\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(data_path))\n",
    "\n",
    "# Visualizando schema e conteúdo\n",
    "print(df_csv_schema.printSchema())\n",
    "df_csv_schema.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.types.StructType"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(flights_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pyspark-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ca62b13d571fed8f86e7643e8b936bf7110cd589ea2bbb5d8933a4a219eedc3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/26 21:24:38 WARN Utils: Your hostname, panini-ubuntu resolves to a loopback address: 127.0.1.1, but we couldn't find any external IP address!\n",
      "22/07/26 21:24:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/26 21:24:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/07/26 21:24:41 WARN MacAddressUtil: Failed to find a usable hardware address from the network interfaces; using random bytes: b4:3f:00:99:ed:9c:13:82\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://panini-ubuntu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>art09-spark-sql</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f99f019b400>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc\n",
    "import os\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "# Definindo variáveis de diretório\n",
    "data_path = os.path.join(''.join(os.path.pardir + \"/\") * 3, 'data/flights-data/summary-data/parquet/2010-summary.parquet/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet')\n",
    "\n",
    "# Inicializando sessão\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"art09-spark-sql\")\n",
    "    .getOrCreate())\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando database via SparkSQL (opcional)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS paninitlab\")\n",
    "spark.sql(\"USE paninitlab\")\n",
    "\n",
    "# Criando tabela\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS flights_data (\n",
    "        DEST_COUNTRY_NAME STRING,\n",
    "        ORIGIN_COUNTRY_NAME STRING,\n",
    "        count INT\n",
    "    )\n",
    "    \n",
    "    USING csv OPTIONS (PATH '/home/hadoop/dev/panini-tech-lab/data/flights-data/summary-data/csv/2015-summary.csv')\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando database via SparkSQL (opcional)\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS paninitlab\")\n",
    "spark.sql(\"USE paninitlab\")\n",
    "\n",
    "# Dropando e criando tabela\n",
    "spark.sql(\"DROP TABLE IF EXISTS flights_data\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS flights_data (\n",
    "        DEST_COUNTRY_NAME STRING,\n",
    "        ORIGIN_COUNTRY_NAME STRING,\n",
    "        count INT\n",
    "    )\n",
    "\n",
    "    -- Definindo formato de origem\n",
    "    USING csv\n",
    "\n",
    "    -- Definindo opções\n",
    "    LOCATION '/home/hadoop/dev/panini-tech-lab/data/flights-data/summary-data/csv/2015-summary.csv'\n",
    "    OPTIONS (\n",
    "        header=\"true\"\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Selecionando dados\n",
    "df_flights_external = spark.sql(\"\"\"\n",
    "    SELECT * FROM flights_data LIMIT 5\n",
    "\"\"\")\n",
    "df_flights_external.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Lendo dados\n",
    "csv_path = '/home/hadoop/dev/panini-tech-lab/data/flights-data/summary-data/csv/2015-summary.csv'\n",
    "df_flights = spark.read.format(\"csv\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(csv_path)\n",
    "\n",
    "# Criando tabela temporária (view)\n",
    "df_flights.createOrReplaceTempView(\"vw_flights_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Consultando view\n",
    "df_flights_view = spark.sql(\"\"\"\n",
    "    SELECT * FROM vw_flights_data LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "# Visualizando dados\n",
    "df_flights_view.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Column(name='DEST_COUNTRY_NAME', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='ORIGIN_COUNTRY_NAME', description=None, dataType='string', nullable=True, isPartition=False, isBucket=False),\n",
       " Column(name='count', description=None, dataType='int', nullable=True, isPartition=False, isBucket=False)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizando databases\n",
    "spark.catalog.listDatabases()\n",
    "\n",
    "# Visualizando tabelas\n",
    "spark.catalog.listTables()\n",
    "\n",
    "# Visualizando colunas de uma tabelas\n",
    "spark.catalog.listColumns(\"flights_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='flights_data', database='paninitlab', description=None, tableType='EXTERNAL', isTemporary=False),\n",
       " Table(name='vw_flights_data', database=None, description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualizando tabelas\n",
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flights.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|           Canada|      United States|  8399|\n",
      "|           Mexico|      United States|  7140|\n",
      "|   United Kingdom|      United States|  2025|\n",
      "|            Japan|      United States|  1548|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Principais destinos de vôos americanos\n",
    "df_flights_eua_ordered = (df_flights.where(\"ORIGIN_COUNTRY_NAME = 'United States'\")\n",
    "                            .orderBy(desc(\"count\")))\n",
    "df_flights_eua_ordered.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#51 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count#51 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#187]\n",
      "      +- Filter (isnotnull(ORIGIN_COUNTRY_NAME#50) AND (ORIGIN_COUNTRY_NAME#50 = United States))\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#49,ORIGIN_COUNTRY_NAME#50,count#51] Batched: false, DataFilters: [isnotnull(ORIGIN_COUNTRY_NAME#50), (ORIGIN_COUNTRY_NAME#50 = United States)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/hadoop/dev/panini-tech-lab/data/flights-data/summary-data/c..., PartitionFilters: [], PushedFilters: [IsNotNull(ORIGIN_COUNTRY_NAME), EqualTo(ORIGIN_COUNTRY_NAME,United States)], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_flights_eua_ordered.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| count|\n",
      "+-----------------+-------------------+------+\n",
      "|    United States|      United States|370002|\n",
      "|           Canada|      United States|  8399|\n",
      "|           Mexico|      United States|  7140|\n",
      "|   United Kingdom|      United States|  2025|\n",
      "|            Japan|      United States|  1548|\n",
      "+-----------------+-------------------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Principais destinos de vôos americanos (SparkSQL)\n",
    "df_sql = spark.sql(\"\"\"\n",
    "    SELECT * FROM vw_flights_data\n",
    "    WHERE ORIGIN_COUNTRY_NAME = 'United States'\n",
    "    ORDER BY count DESC\n",
    "\"\"\")\n",
    "\n",
    "# Visualizando dados\n",
    "df_sql.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [count#51 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(count#51 DESC NULLS LAST, 200), ENSURE_REQUIREMENTS, [id=#217]\n",
      "      +- Filter (isnotnull(ORIGIN_COUNTRY_NAME#50) AND (ORIGIN_COUNTRY_NAME#50 = United States))\n",
      "         +- FileScan csv [DEST_COUNTRY_NAME#49,ORIGIN_COUNTRY_NAME#50,count#51] Batched: false, DataFilters: [isnotnull(ORIGIN_COUNTRY_NAME#50), (ORIGIN_COUNTRY_NAME#50 = United States)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/hadoop/dev/panini-tech-lab/data/flights-data/summary-data/c..., PartitionFilters: [], PushedFilters: [IsNotNull(ORIGIN_COUNTRY_NAME), EqualTo(ORIGIN_COUNTRY_NAME,United States)], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualizando plano de execução\n",
    "df_sql.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flights.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------+\n",
      "|  pais_origem| pais_destino|qtd_voos|\n",
      "+-------------+-------------+--------+\n",
      "|United States|United States|  370002|\n",
      "+-------------+-------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Vôos com origem e destino nos EUA\n",
    "df1 = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        ORIGIN_COUNTRY_NAME AS pais_origem,\n",
    "        DEST_COUNTRY_NAME AS pais_destino,\n",
    "        count AS qtd_voos\n",
    "\n",
    "    FROM vw_flights_data\n",
    "\n",
    "    WHERE ORIGIN_COUNTRY_NAME = 'United States'\n",
    "        AND DEST_COUNTRY_NAME = 'United States'\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|qtd_destinos|\n",
      "+------------+\n",
      "|         131|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Quantidade de destinos\n",
    "df2 = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        count(1) AS qtd_destinos\n",
    "\n",
    "    FROM (\n",
    "        SELECT\n",
    "            DEST_COUNTRY_NAME\n",
    "        \n",
    "        FROM vw_flights_data\n",
    "\n",
    "        WHERE ORIGIN_COUNTRY_NAME = 'United States'\n",
    "            AND DEST_COUNTRY_NAME != 'United States'\n",
    "    )\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flights.where(\"ORIGIN_COUNTRY_NAME = 'United States'\").where(\"DEST_COUNTRY_NAME != 'United States'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------------+-----------------------+-----------------------+-------------------+\n",
      "| pais_destino|       pais_origem|qtd_destino_para_origem|qtd_origem_para_destino|diff_ida_para_volta|\n",
      "+-------------+------------------+-----------------------+-----------------------+-------------------+\n",
      "|United States|             China|                    772|                    920|                148|\n",
      "|United States|            Canada|                   8399|                   8483|                 84|\n",
      "|United States|         Hong Kong|                    332|                    414|                 82|\n",
      "|United States|Dominican Republic|                   1353|                   1420|                 67|\n",
      "|United States|              Peru|                    279|                    337|                 58|\n",
      "+-------------+------------------+-----------------------+-----------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Retornando top 5 países com maior diferença entre viagens de ida para os EUA do que viagens de volta\n",
    "df3 = spark.sql(\"\"\"\n",
    "    WITH american_flights_destination AS (\n",
    "        SELECT\n",
    "            ORIGIN_COUNTRY_NAME,\n",
    "            DEST_COUNTRY_NAME,\n",
    "            count\n",
    "\n",
    "        FROM vw_flights_data\n",
    "\n",
    "        WHERE DEST_COUNTRY_NAME = 'United States'\n",
    "            AND ORIGIN_COUNTRY_NAME != 'United States' \n",
    "    ),\n",
    "\n",
    "    american_flights_origin AS (\n",
    "        SELECT\n",
    "            ORIGIN_COUNTRY_NAME,\n",
    "            DEST_COUNTRY_NAME,\n",
    "            count\n",
    "\n",
    "        FROM vw_flights_data\n",
    "\n",
    "        WHERE ORIGIN_COUNTRY_NAME = 'United States'\n",
    "            AND DEST_COUNTRY_NAME != 'United States' \n",
    "    )\n",
    "\n",
    "    -- Países com maior diferença entre viagens de ida e volta\n",
    "    SELECT\n",
    "        d.DEST_COUNTRY_NAME AS pais_destino,\n",
    "        d.ORIGIN_COUNTRY_NAME AS pais_origem,\n",
    "        o.count AS qtd_destino_para_origem,\n",
    "        d.count AS qtd_origem_para_destino,\n",
    "        (d.count - o.count) AS diff_ida_para_volta\n",
    "    \n",
    "    FROM american_flights_destination AS d\n",
    "\n",
    "    INNER JOIN american_flights_origin AS o\n",
    "        ON (d.ORIGIN_COUNTRY_NAME = o.DEST_COUNTRY_NAME)\n",
    "\n",
    "    ORDER BY diff_ida_para_volta DESC\n",
    "\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyspark-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3a59d43698a1c21b8ea695e9dada7fb9edd9002f05e8c3363eca03517868e40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

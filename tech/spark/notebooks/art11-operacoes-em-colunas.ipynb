{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/03 20:51:49 WARN Utils: Your hostname, panini-ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.0.110 instead (on interface enp3s0)\n",
      "22/08/03 20:51:49 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/08/03 20:52:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import os\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "# Criando sessão Spark\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"art11\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Definindo variáveis de diretório\n",
    "home_path = os.path.expanduser('~')\n",
    "data_path = os.path.join(home_path, 'dev/panini-tech-lab/data/flights-data/summary-data/csv/2015-summary.csv')\n",
    "\n",
    "# Definindo schema para o arquivo CSV a ser lido\n",
    "data_schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), nullable=True, metadata={\"description\": \"País de destino dos vôos contabilizados\"}),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), nullable=True, metadata={\"description\": \"País de origem dos vôos contabilizados\"}),\n",
    "    StructField(\"count\", IntegerType(), nullable=True, metadata={\"description\": \"Contagem total de vôos entre os países de origem e de destino do registro\"})\n",
    "])\n",
    "\n",
    "# Realizando a leitura dos dados\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .schema(data_schema)\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(data_path)\n",
    ")\n",
    "\n",
    "# Criando tabela temporária\n",
    "df.createOrReplaceTempView(\"tbl_flights\")\n",
    "\n",
    "# Verificando amostra dos dados\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|double_count|\n",
      "+-----------------+-------------------+-----+------------+\n",
      "|    United States|            Romania|   15|          30|\n",
      "|    United States|            Croatia|    1|           2|\n",
      "|    United States|            Ireland|  344|         688|\n",
      "|            Egypt|      United States|   15|          30|\n",
      "|    United States|              India|   62|         124|\n",
      "+-----------------+-------------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções adicionais\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Adicionando coluna\n",
    "df_double_count = df.withColumn(\"double_count\", expr(\"count * 2\"))\n",
    "\n",
    "# Visualizando resultado\n",
    "df_double_count.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----+------------+\n",
      "|pais_origem  |pais_destino |count|triple_count|\n",
      "+-------------+-------------+-----+------------+\n",
      "|Romania      |United States|15   |45          |\n",
      "|Croatia      |United States|1    |3           |\n",
      "|Ireland      |United States|344  |1032        |\n",
      "|United States|Egypt        |15   |45          |\n",
      "|India        |United States|62   |186         |\n",
      "+-------------+-------------+-----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformando e consultando\n",
    "df.withColumn(\"triple_count\", col(\"count\") * 3)\\\n",
    "    .select(\n",
    "        col(\"ORIGIN_COUNTRY_NAME\").alias(\"pais_origem\"),\n",
    "        expr(\"DEST_COUNTRY_NAME AS pais_destino\"),\n",
    "        \"count\",\n",
    "        \"triple_count\"\n",
    "    ).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------+\n",
      "| pais_destino|  pais_origem|qtd_voos|\n",
      "+-------------+-------------+--------+\n",
      "|United States|      Romania|      15|\n",
      "|United States|      Croatia|       1|\n",
      "|United States|      Ireland|     344|\n",
      "|        Egypt|United States|      15|\n",
      "|United States|        India|      62|\n",
      "+-------------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomeando colunas\n",
    "df_renamed = df.withColumnRenamed(\"DEST_COUNTRY_NAME\", \"pais_destino\")\\\n",
    "    .withColumnRenamed(\"ORIGIN_COUNTRY_NAME\", \"pais_origem\")\\\n",
    "    .withColumnRenamed(\"count\", \"qtd_voos\")\n",
    "\n",
    "# Visualizando novo DataFrame\n",
    "df_renamed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------+\n",
      "| pais_destino|  pais_origem|contagem|\n",
      "+-------------+-------------+--------+\n",
      "|United States|      Romania|      15|\n",
      "|United States|      Croatia|       1|\n",
      "|United States|      Ireland|     344|\n",
      "|        Egypt|United States|      15|\n",
      "|United States|        India|      62|\n",
      "+-------------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomando colunas através de consultas\n",
    "df_renamed_select = df.select(\n",
    "    col(\"DEST_COUNTRY_NAME\").alias(\"pais_destino\"),\n",
    "    expr(\"ORIGIN_COUNTRY_NAME AS pais_origem\"),\n",
    "    expr(\"count AS contagem\")\n",
    ")\n",
    "\n",
    "# Visualizando resultado\n",
    "df_renamed_select.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----------+\n",
      "| pais_destino|  pais_origem|qtd_viagens|\n",
      "+-------------+-------------+-----------+\n",
      "|United States|      Romania|         15|\n",
      "|United States|      Croatia|          1|\n",
      "|United States|      Ireland|        344|\n",
      "|        Egypt|United States|         15|\n",
      "|United States|        India|         62|\n",
      "+-------------+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renomeando colunas com SparkSQL\n",
    "df_renamed_sql = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        DEST_COUNTRY_NAME AS pais_destino,\n",
    "        ORIGIN_COUNTRY_NAME AS pais_origem,\n",
    "        count AS qtd_viagens\n",
    "\n",
    "    FROM tbl_flights\n",
    "\"\"\")\n",
    "\n",
    "# Visualizando resultado\n",
    "df_renamed_sql.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "+-----------------+-------------------+\n",
      "|    United States|            Romania|\n",
      "|    United States|            Croatia|\n",
      "|    United States|            Ireland|\n",
      "|            Egypt|      United States|\n",
      "|    United States|              India|\n",
      "+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminando colunas\n",
    "df_dropped = df.drop(\"count\")\n",
    "\n",
    "# Visualizando\n",
    "df_dropped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop(*[\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\"]).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----+\n",
      "|count|\n",
      "+-----+\n",
      "|   15|\n",
      "|    1|\n",
      "|  344|\n",
      "|   15|\n",
      "|   62|\n",
      "+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminando múltiplas colunas\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").show(5)\n",
    "\n",
    "# Forma alternativa\n",
    "to_drop = [\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\"]\n",
    "df.drop(*to_drop).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyspark-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3a59d43698a1c21b8ea695e9dada7fb9edd9002f05e8c3363eca03517868e40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

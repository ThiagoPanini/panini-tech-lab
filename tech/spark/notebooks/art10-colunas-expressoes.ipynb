{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import os\n",
    "\n",
    "# Criando objeto de sessão\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"art10-colunas-expressoes\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Definindo variáveis de diretório\n",
    "home_path = os.path.expanduser('~')\n",
    "data_path = os.path.join(home_path, 'dev/panini-tech-lab/data/flights-data/summary-data/csv/2015-summary.csv')\n",
    "\n",
    "# Definindo schema para o arquivo CSV a ser lido\n",
    "data_schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), nullable=True, metadata={\"description\": \"País de destino dos vôos contabilizados\"}),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), nullable=True, metadata={\"description\": \"País de origem dos vôos contabilizados\"}),\n",
    "    StructField(\"count\", IntegerType(), nullable=True, metadata={\"description\": \"Contagem total de vôos entre os países de origem e de destino do registro\"})\n",
    "])\n",
    "\n",
    "# Realizando a leitura dos dados\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .schema(data_schema)\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(data_path)\n",
    ")\n",
    "\n",
    "# Verificando amostra dos dados\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registros (Row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizando uma linha do DataFrame\n",
    "row = df.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15)\n",
      "<class 'pyspark.sql.types.Row'>\n"
     ]
    }
   ],
   "source": [
    "# Visualizando objeto \n",
    "print(row)\n",
    "print(type(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "País de origem: Romania\n",
      "País de destino: United States\n",
      "Contagem de vôos: 15\n"
     ]
    }
   ],
   "source": [
    "# Extraindo informações de um registro\n",
    "print(f'País de origem: {row.ORIGIN_COUNTRY_NAME}')\n",
    "print(f'País de destino: {row.DEST_COUNTRY_NAME}')\n",
    "print(f'Contagem de vôos: {row[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Três primeiros registros: \n",
      "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]\n",
      "\n",
      "Alguns registros específicos: \n",
      "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1), Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Grenada', count=62), Row(DEST_COUNTRY_NAME='Costa Rica', ORIGIN_COUNTRY_NAME='United States', count=588), Row(DEST_COUNTRY_NAME='Senegal', ORIGIN_COUNTRY_NAME='United States', count=40), Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]\n"
     ]
    }
   ],
   "source": [
    "# Coletando as primeiras linhas de um DataFrame\n",
    "n_rows = df.take(3)\n",
    "print(f'Três primeiros registros: \\n{n_rows}')\n",
    "\n",
    "# Coletando todas as linhas de um DataFrame\n",
    "all_rows = df.collect()\n",
    "print(f'\\nAlguns registros específicos: \\n{all_rows[5:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'DEST_COUNTRY_NAME'>\n",
      "Column<'DEST_COUNTRY_NAME'>\n",
      "<class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "# Retornando coluna de um DataFrame\n",
    "print(df.DEST_COUNTRY_NAME)\n",
    "print(df[\"DEST_COUNTRY_NAME\"])\n",
    "\n",
    "# Visualizando tipo primitivo\n",
    "print(type(df[\"DEST_COUNTRY_NAME\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lista de colunas do DatFrame: ['DEST_COUNTRY_NAME', 'ORIGIN_COUNTRY_NAME', 'count']\n",
      "\n",
      "Indexando colunas: Column<'DEST_COUNTRY_NAME'>\n",
      "\n",
      "Tipo primitivo da coluna: <class 'pyspark.sql.column.Column'>\n"
     ]
    }
   ],
   "source": [
    "# Visualizando colunas\n",
    "print(f'Lista de colunas do DataFrame: {df.columns}')\n",
    "\n",
    "# O que ocorre quando referenciamos colunas?\n",
    "print(f'\\nIndexando colunas: {df[\"DEST_COUNTRY_NAME\"]}')\n",
    "\n",
    "# Tipo primitivo\n",
    "print(f'\\nTipo primitivo da coluna: {type(df[\"DEST_COUNTRY_NAME\"])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column<'(valor_A + valor_B)'>\n",
      "Column<'(valor_A + valor_B)'>\n",
      "Column<'(valor_A + valor_B)'>\n"
     ]
    }
   ],
   "source": [
    "# Importando funções de referenciamento de colunas\n",
    "from pyspark.sql.functions import col, column, expr\n",
    "\n",
    "# Construindo expressões via col\n",
    "print(col(\"valor_A\") + col(\"valor_B\"))\n",
    "\n",
    "# Construindo expressões via column\n",
    "print(column(\"valor_A\") + col(\"valor_B\"))\n",
    "\n",
    "# Construindo expressões via expr\n",
    "print(expr(\"valor_A + valor_B\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'((((some_col + 5) * 200) * 6) < other_col)'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando expressão\n",
    "(((col(\"some_col\") + 5) * 200) - 6) < col(\"other_col\")\n",
    "\n",
    "# Modo alternativo\n",
    "expr(\"(((some_col + 5) * 200) * 6) < other_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'((((some_col + 5) * 200) - 6) < other_col)'>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Criando expressão\n",
    "(((col(\"some_col\") + 5) * 200) - 6) < col(\"other_col\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|    United States|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando coluna\n",
    "df.select(\"DEST_COUNTRY_NAME\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "|    United States|    United States|    United States|    United States|\n",
      "|    United States|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Diferentes formas aplicar uma consulta\n",
    "df.select(\n",
    "    \"DEST_COUNTRY_NAME\",\n",
    "    col(\"DEST_COUNTRY_NAME\"),\n",
    "    column(\"DEST_COUNTRY_NAME\"),\n",
    "    expr(\"DEST_COUNTRY_NAME\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+\n",
      "|count|(count * 2)|(count * 2)|\n",
      "+-----+-----------+-----------+\n",
      "|   15|         30|         30|\n",
      "|    1|          2|          2|\n",
      "+-----+-----------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Exemplificando operações\n",
    "df.select(\n",
    "    \"count\",\n",
    "    col(\"count\") * 2,\n",
    "    expr(\"count * 2\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+--------+\n",
      "|  pais_origem| pais_destino|qtd_voos|\n",
      "+-------------+-------------+--------+\n",
      "|      Romania|United States|      15|\n",
      "|      Croatia|United States|       1|\n",
      "|      Ireland|United States|     344|\n",
      "|United States|        Egypt|      15|\n",
      "|        India|United States|      62|\n",
      "+-------------+-------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adicionando alias às expressões\n",
    "df.select(\n",
    "    col(\"ORIGIN_COUNTRY_NAME\").alias(\"pais_origem\"),\n",
    "    col(\"DEST_COUNTRY_NAME\").alias(\"pais_destino\"),\n",
    "    col(\"count\").alias(\"qtd_voos\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----------------+-------------+\n",
      "|pais_origem_expr|pais_destino_expr|qtd_voos_expr|\n",
      "+----------------+-----------------+-------------+\n",
      "|         Romania|    United States|           15|\n",
      "|         Croatia|    United States|            1|\n",
      "|         Ireland|    United States|          344|\n",
      "|   United States|            Egypt|           15|\n",
      "|           India|    United States|           62|\n",
      "+----------------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando consulta com nomes modificados\n",
    "df.select(\n",
    "    expr(\"ORIGIN_COUNTRY_NAME AS pais_origem_expr\"),\n",
    "    expr(\"DEST_COUNTRY_NAME AS pais_destino_expr\"),\n",
    "    expr(\"count AS qtd_voos_expr\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------+--------------+\n",
      "|origem_para_destino    |qtd_voos|qtd_voos_dobro|\n",
      "+-----------------------+--------+--------------+\n",
      "|Romania > United States|15      |30            |\n",
      "|Croatia > United States|1       |2             |\n",
      "|Ireland > United States|344     |688           |\n",
      "|United States > Egypt  |15      |30            |\n",
      "|India > United States  |62      |124           |\n",
      "+-----------------------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando consulta com nomes modificados\n",
    "df.select(\n",
    "    expr(\"concat(ORIGIN_COUNTRY_NAME, ' > ', DEST_COUNTRY_NAME) AS origem_para_destino\"),\n",
    "    expr(\"count AS qtd_voos\"),\n",
    "    expr(\"count * 2 AS qtd_voos_dobro\")\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+--------+--------------+\n",
      "|origem_para_destino    |qtd_voos|qtd_voos_dobro|\n",
      "+-----------------------+--------+--------------+\n",
      "|Romania > United States|15      |30            |\n",
      "|Croatia > United States|1       |2             |\n",
      "|Ireland > United States|344     |688           |\n",
      "|United States > Egypt  |15      |30            |\n",
      "|India > United States  |62      |124           |\n",
      "+-----------------------+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reconstruindo consulta anterior com selectExpr()\n",
    "df.selectExpr(\n",
    "    \"concat(ORIGIN_COUNTRY_NAME, ' > ', DEST_COUNTRY_NAME) AS origem_para_destino\",\n",
    "    \"count AS qtd_voos\",\n",
    "    \"count * 2 AS qtd_voos_dobro\"\n",
    ").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|   upper_dest|power2_count|\n",
      "+-----------------+-------------------+-----+-------------+------------+\n",
      "|    United States|            Romania|   15|UNITED STATES|       225.0|\n",
      "|    United States|            Croatia|    1|UNITED STATES|         1.0|\n",
      "|    United States|            Ireland|  344|UNITED STATES|    118336.0|\n",
      "|            Egypt|      United States|   15|        EGYPT|       225.0|\n",
      "|    United States|              India|   62|UNITED STATES|      3844.0|\n",
      "+-----------------+-------------------+-----+-------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mais um exemplo de consulta\n",
    "df.selectExpr(\n",
    "    \"*\",\n",
    "    \"upper(DEST_COUNTRY_NAME) AS upper_dest\",\n",
    "    \"power(count, 2) AS power2_count\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column 'ERROR_DEST_COUNTRY_NAME' does not exist. Did you mean one of the following? [DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count]; line 1 pos 0;\n'Project ['ERROR_DEST_COUNTRY_NAME]\n+- Relation [DEST_COUNTRY_NAME#32,ORIGIN_COUNTRY_NAME#33,count#34] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art10-colunas-expressoes.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art10-colunas-expressoes.ipynb#ch0000019?line=0'>1</a>\u001b[0m \u001b[39m# Consultando coluna inexistente\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art10-colunas-expressoes.ipynb#ch0000019?line=1'>2</a>\u001b[0m df\u001b[39m.\u001b[39;49mselectExpr(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art10-colunas-expressoes.ipynb#ch0000019?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mERROR_DEST_COUNTRY_NAME\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art10-colunas-expressoes.ipynb#ch0000019?line=3'>4</a>\u001b[0m )\u001b[39m.\u001b[39mshow(\u001b[39m5\u001b[39m)\n",
      "File \u001b[0;32m~/dev/python/venvs/pyspark-venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2048\u001b[0m, in \u001b[0;36mDataFrame.selectExpr\u001b[0;34m(self, *expr)\u001b[0m\n\u001b[1;32m   2046\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(expr) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(expr[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[1;32m   2047\u001b[0m     expr \u001b[39m=\u001b[39m expr[\u001b[39m0\u001b[39m]  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m-> 2048\u001b[0m jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselectExpr(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jseq(expr))\n\u001b[1;32m   2049\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/dev/python/venvs/pyspark-venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/dev/python/venvs/pyspark-venv/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Column 'ERROR_DEST_COUNTRY_NAME' does not exist. Did you mean one of the following? [DEST_COUNTRY_NAME, ORIGIN_COUNTRY_NAME, count]; line 1 pos 0;\n'Project ['ERROR_DEST_COUNTRY_NAME]\n+- Relation [DEST_COUNTRY_NAME#32,ORIGIN_COUNTRY_NAME#33,count#34] csv\n"
     ]
    }
   ],
   "source": [
    "# Consultando coluna inexistente\n",
    "df.selectExpr(\n",
    "    \"ERROR_DEST_COUNTRY_NAME\"\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'count'>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"count\")aaaaaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|           Brazil|             Brazil|  100|\n",
      "|           Brazil|          Argentina|   50|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando objeto do tipo Row\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Criando linhas de um DataFrame\n",
    "my_rows = [\n",
    "    Row(\"Brazil\", \"Brazil\", 100),\n",
    "    Row(\"Brazil\", \"Argentina\", 50)\n",
    "]\n",
    "\n",
    "# Criando um DataFrame\n",
    "my_df = spark.createDataFrame(my_rows, data_schema)\n",
    "my_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyspark-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3a59d43698a1c21b8ea695e9dada7fb9edd9002f05e8c3363eca03517868e40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando bibliotecas\n",
    "from faker import Faker\n",
    "import random\n",
    "\n",
    "# Instanciando objeto de geração de dados\n",
    "fake = Faker()\n",
    "\n",
    "# Configurando seeds\n",
    "Faker.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Gerando dados aleatórios\n",
    "num_rows = 350\n",
    "fake_dates = [fake.date_between(start_date=\"-100d\", end_date=\"today\") for _ in range(num_rows)]\n",
    "fake_users = [fake.first_name() for _ in range(num_rows)]\n",
    "fake_jobs = [fake.job(max_length=15) for _ in range(num_rows)]\n",
    "fake_file = [fake.file_extension() for _ in range(num_rows)]\n",
    "fake_http_methods = [fake.http_method() for _ in range(num_rows)]\n",
    "fake_servers = [fake.windows_platform_token() for _ in range(num_rows)]\n",
    "fake_requests = [random.randrange(1, 200) for _ in range(num_rows)]\n",
    "\n",
    "# Consolidando dados em um formato amigável para o Spark\n",
    "spark_data = []\n",
    "zipped_list = zip(fake_dates, fake_users, fake_jobs, \\\n",
    "    fake_http_methods, fake_file, fake_servers, \\\n",
    "    fake_requests)\n",
    "    \n",
    "for date, user, job, method, file, server, qtd in zipped_list:\n",
    "    spark_data.append([date, user, job, method, file, server, qtd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 11:00:03 WARN Utils: Your hostname, panini-ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.0.110 instead (on interface enp3s0)\n",
      "22/09/07 11:00:03 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 11:00:12 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "root\n",
      " |-- date: date (nullable = false)\n",
      " |-- person: string (nullable = false)\n",
      " |-- job: string (nullable = false)\n",
      " |-- http_method: string (nullable = false)\n",
      " |-- file_ext: string (nullable = false)\n",
      " |-- server_os: string (nullable = false)\n",
      " |-- total_requests: integer (nullable = false)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+-----------+--------+-----------------------+--------------+\n",
      "|date      |person   |job            |http_method|file_ext|server_os              |total_requests|\n",
      "+----------+---------+---------------+-----------+--------+-----------------------+--------------+\n",
      "|2022-05-30|Mitchell |Arboriculturist|DELETE     |odp     |Windows 98; Win 9x 4.90|137           |\n",
      "|2022-05-30|David    |Air cabin crew |CONNECT    |wav     |Windows NT 6.2         |28            |\n",
      "|2022-05-30|Katie    |Charity officer|DELETE     |avi     |Windows NT 6.1         |81            |\n",
      "|2022-05-30|Jasmine  |Geochemist     |DELETE     |js      |Windows 98             |165           |\n",
      "|2022-05-31|David    |Solicitor      |OPTIONS    |flac    |Windows NT 6.0         |63            |\n",
      "|2022-05-31|Anne     |Stage manager  |HEAD       |webm    |Windows 98             |136           |\n",
      "|2022-05-31|Debbie   |Engineer, site |TRACE      |flac    |Windows NT 4.0         |18            |\n",
      "|2022-05-31|Jack     |Osteopath      |DELETE     |xls     |Windows NT 5.0         |109           |\n",
      "|2022-05-31|William  |Chiropodist    |POST       |wav     |Windows NT 6.1         |130           |\n",
      "|2022-06-01|William  |Games developer|CONNECT    |mov     |Windows NT 6.2         |143           |\n",
      "|2022-06-01|Charles  |Translator     |OPTIONS    |html    |Windows 95             |93            |\n",
      "|2022-06-02|Amy      |Curator        |GET        |png     |Windows NT 5.0         |68            |\n",
      "|2022-06-03|Cindy    |Legal secretary|CONNECT    |docx    |Windows NT 5.2         |98            |\n",
      "|2022-06-03|Christine|Administrator  |DELETE     |txt     |Windows NT 5.01        |98            |\n",
      "|2022-06-03|James    |Herpetologist  |CONNECT    |wav     |Windows NT 5.0         |29            |\n",
      "|2022-06-04|Sheila   |Cytogeneticist |PUT        |js      |Windows 98; Win 9x 4.90|174           |\n",
      "|2022-06-04|Joseph   |Dentist        |POST       |mp3     |Windows NT 6.1         |109           |\n",
      "|2022-06-05|Amanda   |Water engineer |HEAD       |avi     |Windows NT 5.2         |27            |\n",
      "|2022-06-05|Jeffrey  |Airline pilot  |TRACE      |wav     |Windows NT 6.2         |170           |\n",
      "|2022-06-05|Brent    |Legal executive|HEAD       |bmp     |Windows NT 5.2         |185           |\n",
      "+----------+---------+---------------+-----------+--------+-----------------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando módulos do Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField,\\\n",
    "    DateType, StringType, IntegerType\n",
    "\n",
    "# Criando sessão no Spark\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"windowing-functions\")\\\n",
    "    .master(\"local[1]\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definindo schema para os dados\n",
    "sales_schema = StructType([\n",
    "    StructField(\"date\", DateType(), nullable=False),\n",
    "    StructField(\"person\", StringType(), nullable=False),\n",
    "    StructField(\"job\", StringType(), nullable=False),\n",
    "    StructField(\"http_method\", StringType(), nullable=False),\n",
    "    StructField(\"file_ext\", StringType(), nullable=False),\n",
    "    StructField(\"server_os\", StringType(), nullable=False),\n",
    "    StructField(\"total_requests\", IntegerType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Criando DataFrame no Spark\n",
    "df = spark.createDataFrame(data=spark_data, schema=sales_schema)\\\n",
    "    .orderBy(\"date\")\n",
    "\n",
    "# Visualizando dados\n",
    "df.printSchema()\n",
    "df.orderBy(\"date\").show(20, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+\n",
      "|      date|max_requests|\n",
      "+----------+------------+\n",
      "|2022-05-30|         165|\n",
      "|2022-05-31|         136|\n",
      "|2022-06-01|         143|\n",
      "|2022-06-02|          68|\n",
      "|2022-06-03|          98|\n",
      "|2022-06-04|         174|\n",
      "|2022-06-05|         185|\n",
      "|2022-06-06|         143|\n",
      "|2022-06-07|         195|\n",
      "|2022-06-08|         187|\n",
      "+----------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import col, expr, max\n",
    "\n",
    "# Agrupando dados por data\n",
    "df_max_requests = df.groupBy(col(\"date\")).agg(\n",
    "    max(\"total_requests\").alias(\"max_requests\")\n",
    ").orderBy(\"date\")\n",
    "\n",
    "# Visualizando resultado\n",
    "df_max_requests.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------------+-----------+--------+--------------------+--------------+------------+\n",
      "|      date|  person|            job|http_method|file_ext|           server_os|total_requests|max_requests|\n",
      "+----------+--------+---------------+-----------+--------+--------------------+--------------+------------+\n",
      "|2022-05-30| Jasmine|     Geochemist|     DELETE|      js|          Windows 98|           165|         165|\n",
      "|2022-05-30|Mitchell|Arboriculturist|     DELETE|     odp|Windows 98; Win 9...|           137|         165|\n",
      "|2022-05-30|   Katie|Charity officer|     DELETE|     avi|      Windows NT 6.1|            81|         165|\n",
      "|2022-05-30|   David| Air cabin crew|    CONNECT|     wav|      Windows NT 6.2|            28|         165|\n",
      "|2022-05-31|    Anne|  Stage manager|       HEAD|    webm|          Windows 98|           136|         136|\n",
      "|2022-05-31|   David|      Solicitor|    OPTIONS|    flac|      Windows NT 6.0|            63|         136|\n",
      "|2022-05-31| William|    Chiropodist|       POST|     wav|      Windows NT 6.1|           130|         136|\n",
      "|2022-05-31|    Jack|      Osteopath|     DELETE|     xls|      Windows NT 5.0|           109|         136|\n",
      "|2022-05-31|  Debbie| Engineer, site|      TRACE|    flac|      Windows NT 4.0|            18|         136|\n",
      "|2022-06-01| William|Games developer|    CONNECT|     mov|      Windows NT 6.2|           143|         143|\n",
      "+----------+--------+---------------+-----------+--------+--------------------+--------------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplicando join\n",
    "df_max_requests_join = df.join(\n",
    "    other=df_max_requests,\n",
    "    on=[df.date == df_max_requests.date],\n",
    "    how=\"left\"\n",
    ").select(\n",
    "    df[\"*\"],\n",
    "    df_max_requests[\"max_requests\"]\n",
    ").orderBy(\"date\")\n",
    "\n",
    "# Visualizando\n",
    "df_max_requests_join.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------------+-----------+--------+--------------------+--------------+------------------+\n",
      "|      date|  person|            job|http_method|file_ext|           server_os|total_requests|max_daily_requests|\n",
      "+----------+--------+---------------+-----------+--------+--------------------+--------------+------------------+\n",
      "|2022-05-30|   Katie|Charity officer|     DELETE|     avi|      Windows NT 6.1|            81|               165|\n",
      "|2022-05-30| Jasmine|     Geochemist|     DELETE|      js|          Windows 98|           165|               165|\n",
      "|2022-05-30|Mitchell|Arboriculturist|     DELETE|     odp|Windows 98; Win 9...|           137|               165|\n",
      "|2022-05-30|   David| Air cabin crew|    CONNECT|     wav|      Windows NT 6.2|            28|               165|\n",
      "|2022-05-31| William|    Chiropodist|       POST|     wav|      Windows NT 6.1|           130|               136|\n",
      "|2022-05-31|    Anne|  Stage manager|       HEAD|    webm|          Windows 98|           136|               136|\n",
      "|2022-05-31|  Debbie| Engineer, site|      TRACE|    flac|      Windows NT 4.0|            18|               136|\n",
      "|2022-05-31|   David|      Solicitor|    OPTIONS|    flac|      Windows NT 6.0|            63|               136|\n",
      "|2022-05-31|    Jack|      Osteopath|     DELETE|     xls|      Windows NT 5.0|           109|               136|\n",
      "|2022-06-01| William|Games developer|    CONNECT|     mov|      Windows NT 6.2|           143|               143|\n",
      "+----------+--------+---------------+-----------+--------+--------------------+--------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Especificando janela de análise\n",
    "window_date = Window.partitionBy(col(\"date\"))\n",
    "\n",
    "# Selecionando dados\n",
    "df_max_daily_requests = df.select(\n",
    "    \"*\",\n",
    "    max(col(\"total_requests\")).over(window_date).alias(\"max_daily_requests\")\n",
    ")\n",
    "\n",
    "# Visualizando resultados\n",
    "df_max_daily_requests.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------+---------------+-----------+--------+--------------------+--------------+-----------------+----+\n",
      "|      date|  person|            job|http_method|file_ext|           server_os|total_requests|max_daily_request|rank|\n",
      "+----------+--------+---------------+-----------+--------+--------------------+--------------+-----------------+----+\n",
      "|2022-05-30| Jasmine|     Geochemist|     DELETE|      js|          Windows 98|           165|              165|   1|\n",
      "|2022-05-30|Mitchell|Arboriculturist|     DELETE|     odp|Windows 98; Win 9...|           137|              165|   2|\n",
      "|2022-05-30|   Katie|Charity officer|     DELETE|     avi|      Windows NT 6.1|            81|              165|   3|\n",
      "|2022-05-30|   David| Air cabin crew|    CONNECT|     wav|      Windows NT 6.2|            28|              165|   4|\n",
      "|2022-05-31|    Anne|  Stage manager|       HEAD|    webm|          Windows 98|           136|              136|   1|\n",
      "|2022-05-31| William|    Chiropodist|       POST|     wav|      Windows NT 6.1|           130|              136|   2|\n",
      "|2022-05-31|    Jack|      Osteopath|     DELETE|     xls|      Windows NT 5.0|           109|              136|   3|\n",
      "|2022-05-31|   David|      Solicitor|    OPTIONS|    flac|      Windows NT 6.0|            63|              136|   4|\n",
      "|2022-05-31|  Debbie| Engineer, site|      TRACE|    flac|      Windows NT 4.0|            18|              136|   5|\n",
      "|2022-06-01| William|Games developer|    CONNECT|     mov|      Windows NT 6.2|           143|              143|   1|\n",
      "+----------+--------+---------------+-----------+--------+--------------------+--------------+-----------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "# Criando janela ordenada por requisições\n",
    "window_spec = Window\\\n",
    "    .partitionBy(col(\"date\"))\\\n",
    "    .orderBy(col(\"total_requests\").desc())\n",
    "\n",
    "# Criando consulta através da janela especificada\n",
    "df_ranked_requests = df.select(\n",
    "    \"*\",\n",
    "    max(\"total_requests\").over(window_spec).alias(\"max_daily_request\"),\n",
    "    rank().over(window_spec).alias(\"rank\")\n",
    ")\n",
    "\n",
    "# Visualizando resultados\n",
    "df_ranked_requests.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------------+-----------+--------+--------------------+--------------+-----------------+----+\n",
      "|      date|   person|            job|http_method|file_ext|           server_os|total_requests|max_daily_request|rank|\n",
      "+----------+---------+---------------+-----------+--------+--------------------+--------------+-----------------+----+\n",
      "|2022-05-30|  Jasmine|     Geochemist|     DELETE|      js|          Windows 98|           165|              165|   1|\n",
      "|2022-05-31|     Anne|  Stage manager|       HEAD|    webm|          Windows 98|           136|              136|   1|\n",
      "|2022-06-01|  William|Games developer|    CONNECT|     mov|      Windows NT 6.2|           143|              143|   1|\n",
      "|2022-06-02|      Amy|        Curator|        GET|     png|      Windows NT 5.0|            68|               68|   1|\n",
      "|2022-06-03|    Cindy|Legal secretary|    CONNECT|    docx|      Windows NT 5.2|            98|               98|   1|\n",
      "|2022-06-03|Christine|  Administrator|     DELETE|     txt|     Windows NT 5.01|            98|               98|   1|\n",
      "|2022-06-04|   Sheila| Cytogeneticist|        PUT|      js|Windows 98; Win 9...|           174|              174|   1|\n",
      "|2022-06-05|    Brent|Legal executive|       HEAD|     bmp|      Windows NT 5.2|           185|              185|   1|\n",
      "|2022-06-06|  Charles|   Video editor|       HEAD| numbers|     Windows NT 5.01|           143|              143|   1|\n",
      "|2022-06-07|      Lee|  Haematologist|     DELETE|     png|     Windows NT 5.01|           195|              195|   1|\n",
      "+----------+---------+---------------+-----------+--------+--------------------+--------------+-----------------+----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ranked_requests.where(expr(\"rank = 1\")).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incrementar:\n",
    "\n",
    "    - Soma acumulada por mês"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando classe\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Definindo especificação de janela\n",
    "window_spec = Window\\\n",
    "    .partitionBy(\"col_A\")\\\n",
    "    .orderBy(\"col_B\")\n",
    "\n",
    "# Definindo especificação de janela\n",
    "window_spec = Window\\\n",
    "    .partitionBy(\"col_A\")\\\n",
    "    .orderBy(\"col_B\")\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparando Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 11:31:58 WARN Utils: Your hostname, panini-ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.0.110 instead (on interface enp3s0)\n",
      "22/09/07 11:31:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 11:32:08 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "root\n",
      " |-- device_id: integer (nullable = true)\n",
      " |-- device_name: string (nullable = true)\n",
      " |-- ip: string (nullable = true)\n",
      " |-- cca2: string (nullable = true)\n",
      " |-- cca3: string (nullable = true)\n",
      " |-- cn: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- scale: string (nullable = true)\n",
      " |-- temp: integer (nullable = true)\n",
      " |-- humidity: integer (nullable = true)\n",
      " |-- battery_level: string (nullable = true)\n",
      " |-- c02_level: integer (nullable = true)\n",
      " |-- lcd: string (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------------------+---------------+----+----+-------------+--------+---------+-------+----+--------+-------------+---------+------+-------------+\n",
      "|device_id|         device_name|             ip|cca2|cca3|           cn|latitude|longitude|  scale|temp|humidity|battery_level|c02_level|   lcd|    timestamp|\n",
      "+---------+--------------------+---------------+----+----+-------------+--------+---------+-------+----+--------+-------------+---------+------+-------------+\n",
      "|        1|meter-gauge-1xbYRYcj|   68.161.225.1|  US| USA|United States|    38.0|    -97.0|Celsius|  34|      51|            8|      868| green|1458444054093|\n",
      "|        2|   sensor-pad-2n2Pea|  213.161.254.1|  NO| NOR|       Norway|   62.47|     6.15|Celsius|  11|      70|            7|     1473|   red|1458444054119|\n",
      "|        3| device-mac-36TWSKiT|      88.36.5.1|  IT| ITA|        Italy|   42.83|    12.83|Celsius|  19|      44|            2|     1556|   red|1458444054120|\n",
      "|        4|   sensor-pad-4mzWkz|  66.39.173.154|  US| USA|United States|   44.06|  -121.32|Celsius|  28|      32|            6|     1080|yellow|1458444054121|\n",
      "|        5|therm-stick-5gimp...|    203.82.41.9|  PH| PHL|  Philippines|   14.58|   120.97|Celsius|  25|      62|            4|      931| green|1458444054122|\n",
      "|        6|sensor-pad-6al7RT...| 204.116.105.67|  US| USA|United States|   35.93|   -85.46|Celsius|  27|      51|            3|     1210|yellow|1458444054122|\n",
      "|        7|meter-gauge-7GeDoanM|  220.173.179.1|  CN| CHN|        China|   22.82|   108.32|Celsius|  18|      26|            3|     1129|yellow|1458444054123|\n",
      "|        8|sensor-pad-8xUD6p...|  210.173.177.1|  JP| JPN|        Japan|   35.69|   139.69|Celsius|  27|      35|            0|     1536|   red|1458444054123|\n",
      "|        9| device-mac-9GcjZ2pw|  118.23.68.227|  JP| JPN|        Japan|   35.69|   139.69|Celsius|  13|      85|            3|      807| green|1458444054124|\n",
      "|       10|sensor-pad-10Bsyw...|208.109.163.218|  US| USA|United States|   33.61|  -111.89|Celsius|  26|      56|            7|     1470|   red|1458444054125|\n",
      "+---------+--------------------+---------------+----+----+-------------+--------+---------+-------+----+--------+-------------+---------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando biblitoecas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, \\\n",
    "    StringType, IntegerType, DoubleType, LongType\n",
    "import os \n",
    "\n",
    "# Criando objeto de sessão\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"agregacoes\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definindo variáveis de diretório\n",
    "home_path = os.path.expanduser('~')\n",
    "data_path = os.path.join(home_path, 'dev/panini-tech-lab/data')\n",
    "iot_path = os.path.join(data_path, 'iot-devices/iot_devices.json')\n",
    "\n",
    "# Definindo schema para o arquivo a ser lido\n",
    "iot_schema = StructType([\n",
    "    StructField(\"device_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"device_name\", StringType(), nullable=True),\n",
    "    StructField(\"ip\", StringType(), nullable=True),\n",
    "    StructField(\"cca2\", StringType(), nullable=True),\n",
    "    StructField(\"cca3\", StringType(), nullable=True),\n",
    "    StructField(\"cn\", StringType(), nullable=True),\n",
    "    StructField(\"latitude\", DoubleType(), nullable=True),\n",
    "    StructField(\"longitude\", DoubleType(), nullable=True),\n",
    "    StructField(\"scale\", StringType(), nullable=True),\n",
    "    StructField(\"temp\", IntegerType(), nullable=True),\n",
    "    StructField(\"humidity\", IntegerType(), nullable=True),\n",
    "    StructField(\"battery_level\", StringType(), nullable=True),\n",
    "    StructField(\"c02_level\", IntegerType(), nullable=True),\n",
    "    StructField(\"lcd\", StringType(), nullable=True),\n",
    "    StructField(\"timestamp\", LongType(), nullable=False)\n",
    "])\n",
    "\n",
    "# Lendo dados\n",
    "df_iot_raw = spark.read.format(\"json\")\\\n",
    "    .schema(iot_schema)\\\n",
    "    .load(iot_path)\n",
    "\n",
    "# Criando tabelas temporárias\n",
    "df_iot_raw.createOrReplaceTempView(\"tbl_iot\")\n",
    "\n",
    "# Visualizando dados\n",
    "df_iot_raw.printSchema()\n",
    "df_iot_raw.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+----+--------+---------+-------------------+----------+\n",
      "|device_type|      country|temp|humidity|c02_level|          timestamp|      date|\n",
      "+-----------+-------------+----+--------+---------+-------------------+----------+\n",
      "|      therm|United States|  25|      30|     1566|2015-11-25 07:39:33|2015-11-25|\n",
      "|      meter|United States|  33|      43|      965|2015-11-25 07:39:35|2015-11-25|\n",
      "|     sensor|United States|  16|      97|     1385|2015-11-25 07:39:36|2015-11-25|\n",
      "|      meter|United States|  16|      30|     1047|2015-11-25 07:39:38|2015-11-25|\n",
      "|      therm|United States|  17|      70|      816|2015-11-25 07:54:13|2015-11-25|\n",
      "|     device|United States|  31|      62|      834|2015-11-25 07:54:15|2015-11-25|\n",
      "|     sensor|United States|  20|      86|     1184|2015-11-25 07:54:16|2015-11-25|\n",
      "|      meter| Saudi Arabia|  27|      72|     1318|2015-11-25 07:54:18|2015-11-25|\n",
      "|      meter|      Hungary|  17|      42|     1256|2015-11-25 08:02:42|2015-11-25|\n",
      "|      therm|United States|  30|      70|      967|2015-11-25 08:02:43|2015-11-25|\n",
      "+-----------+-------------+----+--------+---------+-------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "import random\n",
    "from pyspark.sql.functions import udf, monotonically_increasing_id,\\\n",
    "    split, to_date, from_unixtime, col, expr\n",
    "\n",
    "# Criando lista de valores aleatórios para serem adicionados ao DataFrame\n",
    "random.seed(42)\n",
    "rows = df_iot_raw.count()\n",
    "date_add = [random.randint(a=-10000000, b=10000000) for i in range(rows)]\n",
    "\n",
    "# Gerando base de dados de trabalho\n",
    "df_iot = df_iot_raw.select(\n",
    "    split(col(\"device_name\"), \"-\")[0].alias(\"device_type\"),\n",
    "    col(\"cn\").alias(\"country\"),\n",
    "    col(\"temp\"),\n",
    "    col(\"humidity\"),\n",
    "    col(\"c02_level\"),\n",
    "    col(\"timestamp\").alias(\"timestamp_raw\")\n",
    ").withColumn(\"date_add\", udf(lambda id: date_add[id])(monotonically_increasing_id()))\\\n",
    "    .withColumn(\"timestamp\", from_unixtime((col(\"timestamp_raw\") / 1000) + col(\"date_add\")))\\\n",
    "    .withColumn(\"date\", to_date(col(\"timestamp\")))\\\n",
    "    .drop(\"timestamp_raw\", \"date_add\")\n",
    "\n",
    "# Visualizando dados\n",
    "df_iot.orderBy(\"timestamp\").show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agregando dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 38:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+------------+\n",
      "|          country|avg_temp|measurements|\n",
      "+-----------------+--------+------------+\n",
      "|    United States|   21.99|       68545|\n",
      "|            China|   21.91|       14455|\n",
      "|            Japan|   22.07|       12100|\n",
      "|Republic of Korea|   22.15|       11879|\n",
      "|          Germany|   21.99|        7942|\n",
      "|   United Kingdom|   22.09|        6486|\n",
      "|           Canada|   21.91|        6041|\n",
      "|           Russia|   22.15|        5989|\n",
      "|           France|   22.12|        5305|\n",
      "|           Brazil|   21.96|        3224|\n",
      "+-----------------+--------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import avg, round, count\n",
    "\n",
    "# Aplicando agrupamento\n",
    "df_country_temp = df_iot.groupBy(\"country\").agg(\n",
    "    round(avg(col(\"temp\")), 2).alias(\"avg_temp\"),\n",
    "    count(\"*\").alias(\"measurements\")\n",
    ")\n",
    "\n",
    "# Visualizando resultado\n",
    "df_country_temp.orderBy(\"measurements\", ascending=False).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:=============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 11:58:52 WARN DAGScheduler: Broadcasting large task binary with size 1012.2 KiB\n",
      "+-------------+----+--------+---------+----------+-----------+-------+\n",
      "|      country|temp|humidity|c02_level|      date|avg_cn_temp|cn_meas|\n",
      "+-------------+----+--------+---------+----------+-----------+-------+\n",
      "|       France|  14|      85|     1201|2016-01-25|      22.12|   5305|\n",
      "|United States|  12|      91|      931|2016-03-25|      21.99|  68545|\n",
      "|United States|  33|      70|     1336|2016-02-10|      21.99|  68545|\n",
      "|United States|  27|      89|     1584|2015-12-15|      21.99|  68545|\n",
      "|        Japan|  25|      40|     1350|2016-02-06|      22.07|  12100|\n",
      "|        China|  26|      28|     1560|2015-12-13|      21.91|  14455|\n",
      "|        China|  28|      87|     1532|2016-02-03|      21.91|  14455|\n",
      "|    Australia|  25|      38|     1376|2016-03-21|      21.91|   3119|\n",
      "|United States|  13|      85|      876|2016-05-25|      21.99|  68545|\n",
      "|United States|  17|      26|      862|2016-02-29|      21.99|  68545|\n",
      "+-------------+----+--------+---------+----------+-----------+-------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando módulos\n",
    "from pyspark.sql import Window\n",
    "\n",
    "# Definindo especificação de janela\n",
    "window_spec = Window.partitionBy(\"country\")\n",
    "\n",
    "# Aplicando consulta\n",
    "df_country_temp_window = df_iot\\\n",
    "    .where(expr(\"country != ''\"))\\\n",
    "        .select(\n",
    "        \"country\",\n",
    "        \"temp\",\n",
    "        \"humidity\",\n",
    "        \"c02_level\",\n",
    "        \"date\",\n",
    "        round(avg(\"temp\").over(window_spec), 2).alias(\"avg_cn_temp\"),\n",
    "        count(\"*\").over(window_spec).alias(\"cn_meas\")\n",
    "    )\n",
    "\n",
    "# Importando função para ordenação\n",
    "from pyspark.sql.functions import rand\n",
    "\n",
    "# Visualizando resultado\n",
    "df_country_temp_window.orderBy(rand()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exemplos Práticos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### rank() e dense_rank()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CORRIGIR WINDOWING POR DATA PARA EVITAR O AGRUPAMENTO POR DATA...\n",
    "\n",
    "A INTENÇÃO É TRAZER O MÁXIMO CONSIDERANDO A BASE INTEIRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 12:03:03 WARN DAGScheduler: Broadcasting large task binary with size 1010.3 KiB\n",
      "+-----------+-----------------+---------+----------+-------------------+--------+--------------+\n",
      "|device_type|          country|c02_level|      date|          timestamp|c02_rank|c02_dense_rank|\n",
      "+-----------+-----------------+---------+----------+-------------------+--------+--------------+\n",
      "|     sensor|    United States|     1599|2015-11-25|2015-11-25 18:56:04|       1|             1|\n",
      "|     device|            China|     1599|2015-11-25|2015-11-25 17:26:27|       1|             1|\n",
      "|     sensor|    United States|     1598|2015-11-25|2015-11-25 21:16:20|       3|             2|\n",
      "|      therm|            Japan|     1597|2015-11-25|2015-11-25 14:58:09|       4|             3|\n",
      "|     sensor|   United Kingdom|     1596|2015-11-25|2015-11-25 16:58:48|       5|             4|\n",
      "|     sensor|            China|     1596|2015-11-25|2015-11-25 21:06:39|       5|             4|\n",
      "|     sensor|      Netherlands|     1596|2015-11-25|2015-11-25 10:05:44|       5|             4|\n",
      "|      therm|Republic of Korea|     1596|2015-11-25|2015-11-25 13:40:25|       5|             4|\n",
      "|      meter|          Germany|     1594|2015-11-25|2015-11-25 13:40:21|       9|             5|\n",
      "|     device|    United States|     1594|2015-11-25|2015-11-25 20:16:54|       9|             5|\n",
      "+-----------+-----------------+---------+----------+-------------------+--------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Especificando janela\n",
    "window_date_c02 = Window\\\n",
    "    .partitionBy(\"date\")\\\n",
    "    .orderBy(col(\"c02_level\").desc())\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Importando funções\n",
    "from pyspark.sql.functions import rank, dense_rank\n",
    "\n",
    "# Consultando datas de pico de emissão de carbono\n",
    "df_c02_peak = df_iot.select(\n",
    "    col(\"device_type\"),\n",
    "    col(\"country\"),\n",
    "    col(\"c02_level\"),\n",
    "    col(\"date\"),\n",
    "    col(\"timestamp\"),\n",
    "    rank().over(window_date_c02).alias(\"c02_rank\"),\n",
    "    dense_rank().over(window_date_c02).alias(\"c02_dense_rank\")\n",
    ")\n",
    "\n",
    "# Visualizando dados\n",
    "df_c02_peak.orderBy(\"date\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 82:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 14:00:12 WARN DAGScheduler: Broadcasting large task binary with size 1014.0 KiB\n",
      "+-----------+-------------+---------+----------+-------------------+--------+--------------+\n",
      "|device_type|      country|c02_level|      date|          timestamp|c02_rank|c02_dense_rank|\n",
      "+-----------+-------------+---------+----------+-------------------+--------+--------------+\n",
      "|     sensor|United States|     1599|2015-11-25|2015-11-25 18:56:04|       1|             1|\n",
      "|     sensor|United States|     1598|2015-11-25|2015-11-25 21:16:20|       3|             2|\n",
      "|     device|        China|     1599|2015-11-25|2015-11-25 17:26:27|       1|             1|\n",
      "|     sensor|United States|     1599|2015-11-26|2015-11-26 03:39:42|       1|             1|\n",
      "|     device|        China|     1599|2015-11-26|2015-11-26 17:59:50|       1|             1|\n",
      "|      meter|        China|     1599|2015-11-26|2015-11-26 12:05:08|       1|             1|\n",
      "|     sensor|United States|     1598|2015-11-27|2015-11-27 21:21:44|       3|             2|\n",
      "|     sensor|       Russia|     1599|2015-11-27|2015-11-27 01:47:17|       1|             1|\n",
      "|     sensor|       Norway|     1599|2015-11-27|2015-11-27 03:14:53|       1|             1|\n",
      "|     sensor|United States|     1598|2015-11-27|2015-11-27 16:36:26|       3|             2|\n",
      "|      therm|United States|     1597|2015-11-28|2015-11-28 00:54:50|       3|             3|\n",
      "|     sensor|United States|     1599|2015-11-28|2015-11-28 00:01:47|       1|             1|\n",
      "|     sensor|United States|     1598|2015-11-28|2015-11-28 14:10:26|       2|             2|\n",
      "|     device|United States|     1599|2015-11-29|2015-11-29 16:00:02|       1|             1|\n",
      "|     sensor|       Taiwan|     1599|2015-11-29|2015-11-29 06:22:18|       1|             1|\n",
      "+-----------+-------------+---------+----------+-------------------+--------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Top 3 ofensores por dia\n",
    "df_c02_peak\\\n",
    "    .where(expr(\"c02_rank <= 3\"))\\\n",
    "    .orderBy(\"date\")\\\n",
    "    .show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 115:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 14:17:55 WARN DAGScheduler: Broadcasting large task binary with size 1013.3 KiB\n",
      "22/09/07 14:17:56 WARN DAGScheduler: Broadcasting large task binary with size 1013.6 KiB\n",
      "+-----------------+----------------------+--------------------+\n",
      "|          country|count_main_c02_emissor| c02_peak_timestamps|\n",
      "+-----------------+----------------------+--------------------+\n",
      "|    United States|                   132|[2016-01-30 19:12...|\n",
      "|            China|                    37|[2016-01-31 16:15...|\n",
      "|            Japan|                    25|[2016-02-03 15:14...|\n",
      "|Republic of Korea|                    23|[2015-12-29 03:28...|\n",
      "|          Germany|                    17|[2016-04-05 19:41...|\n",
      "|   United Kingdom|                    12|[2015-12-16 13:59...|\n",
      "|           Russia|                    12|[2016-04-09 20:36...|\n",
      "|           Canada|                    10|[2016-01-30 15:50...|\n",
      "|           Brazil|                    10|[2016-01-13 01:53...|\n",
      "|           Sweden|                     7|[2016-01-23 09:42...|\n",
      "+-----------------+----------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import collect_list, collect_set\n",
    "\n",
    "# Quantidade de vezes que um país ficou no top 1\n",
    "df_c02_peak\\\n",
    "    .where(expr(\"c02_rank == 1\"))\\\n",
    "    .groupBy(\"country\").agg(\n",
    "        count(\"*\").alias(\"count_main_c02_emissor\"),\n",
    "        collect_list(\"timestamp\").alias(\"c02_peak_timestamps\")\n",
    "    )\\\n",
    "    .orderBy(\"count_main_c02_emissor\", ascending=False)\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### percent_rank() e ntile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 215:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 16:07:12 WARN DAGScheduler: Broadcasting large task binary with size 1018.7 KiB\n",
      "+-----------+-----------------+----+--------+---------+----------+----+----------+------------+---------+\n",
      "|device_type|          country|temp|humidity|c02_level|      date|rank|dense_rank|percent_rank|ntile_100|\n",
      "+-----------+-----------------+----+--------+---------+----------+----+----------+------------+---------+\n",
      "|     sensor|    United States|  29|      98|     1599|2015-11-25|   1|         1|         0.0|        1|\n",
      "|     device|            China|  24|      80|     1599|2015-11-25|   1|         1|         0.0|        1|\n",
      "|     sensor|    United States|  20|      57|     1598|2015-11-25|   3|         2|     0.00338|        1|\n",
      "|      therm|            Japan|  33|      43|     1597|2015-11-25|   4|         3|     0.00507|        1|\n",
      "|     sensor|            China|  23|      34|     1596|2015-11-25|   5|         4|     0.00676|        1|\n",
      "|     sensor|   United Kingdom|  32|      66|     1596|2015-11-25|   5|         4|     0.00676|        1|\n",
      "|     sensor|      Netherlands|  29|      88|     1596|2015-11-25|   5|         4|     0.00676|        2|\n",
      "|      therm|Republic of Korea|  16|      71|     1596|2015-11-25|   5|         4|     0.00676|        2|\n",
      "|      meter|          Germany|  21|      66|     1594|2015-11-25|   9|         5|     0.01351|        2|\n",
      "|     device|    United States|  12|      94|     1594|2015-11-25|   9|         5|     0.01351|        2|\n",
      "|      therm|    United States|  25|      73|     1590|2015-11-25|  11|         6|     0.01689|        2|\n",
      "|     sensor|   Czech Republic|  23|      36|     1589|2015-11-25|  12|         7|     0.01858|        2|\n",
      "|      meter|           Russia|  33|      98|     1589|2015-11-25|  12|         7|     0.01858|        3|\n",
      "|     sensor|           Taiwan|  30|      27|     1588|2015-11-25|  14|         8|     0.02196|        3|\n",
      "|      meter|    United States|  24|      81|     1587|2015-11-25|  15|         9|     0.02365|        3|\n",
      "+-----------+-----------------+----+--------+---------+----------+----+----------+------------+---------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import percent_rank, ntile, round\n",
    "\n",
    "# Comparando diferentes funções de ranqueamento\n",
    "df_c02_peak_ranking = df_iot.select(\n",
    "    \"device_type\",\n",
    "    \"country\",\n",
    "    \"temp\",\n",
    "    \"humidity\",\n",
    "    \"c02_level\",\n",
    "    \"date\",\n",
    "    rank().over(window_date_c02).alias(\"rank\"),\n",
    "    dense_rank().over(window_date_c02).alias(\"dense_rank\"),\n",
    "    round(percent_rank().over(window_date_c02), 5).alias(\"percent_rank\"),\n",
    "    ntile(n=100).over(window_date_c02).alias(\"ntile_100\")\n",
    ")\n",
    "\n",
    "# Visualizando dados\n",
    "df_c02_peak_ranking.orderBy([\"date\", \"rank\"]).show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 16:08:07 WARN DAGScheduler: Broadcasting large task binary with size 1001.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 218:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 16:08:09 WARN DAGScheduler: Broadcasting large task binary with size 1021.9 KiB\n",
      "+-----------+-----------------+----+--------+---------+----------+----+----------+------------+---------+\n",
      "|device_type|          country|temp|humidity|c02_level|      date|rank|dense_rank|percent_rank|ntile_100|\n",
      "+-----------+-----------------+----+--------+---------+----------+----+----------+------------+---------+\n",
      "|     sensor|    United States|  29|      98|     1599|2015-11-25|   1|         1|         0.0|        1|\n",
      "|     device|            China|  24|      80|     1599|2015-11-25|   1|         1|         0.0|        1|\n",
      "|     sensor|    United States|  20|      57|     1598|2015-11-25|   3|         2|     0.00338|        1|\n",
      "|      therm|            Japan|  33|      43|     1597|2015-11-25|   4|         3|     0.00507|        1|\n",
      "|     sensor|   United Kingdom|  32|      66|     1596|2015-11-25|   5|         4|     0.00676|        1|\n",
      "|     sensor|            China|  23|      34|     1596|2015-11-25|   5|         4|     0.00676|        1|\n",
      "|     sensor|      Netherlands|  29|      88|     1596|2015-11-25|   5|         4|     0.00676|        2|\n",
      "|      therm|Republic of Korea|  16|      71|     1596|2015-11-25|   5|         4|     0.00676|        2|\n",
      "+-----------+-----------------+----+--------+---------+----------+----+----------+------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Top 1% países mais emissores de um dia específico\n",
    "df_c02_peak_ranking\\\n",
    "    .where(expr(\"date = '2015-11-25'\"))\\\n",
    "    .where(expr(\"percent_rank <= 0.01\"))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 16:08:20 WARN DAGScheduler: Broadcasting large task binary with size 1001.0 KiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 221:>                                                        (0 + 4) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 16:08:21 WARN DAGScheduler: Broadcasting large task binary with size 1020.4 KiB\n",
      "+-----------+---------------+----+--------+---------+----------+----+----------+------------+---------+\n",
      "|device_type|        country|temp|humidity|c02_level|      date|rank|dense_rank|percent_rank|ntile_100|\n",
      "+-----------+---------------+----+--------+---------+----------+----+----------+------------+---------+\n",
      "|      meter|  United States|  33|      85|      801|2015-11-25| 592|       426|     0.99831|      100|\n",
      "|      meter|Slovak Republic|  15|      77|      801|2015-11-25| 592|       426|     0.99831|      100|\n",
      "|     sensor|  United States|  12|      29|      802|2015-11-25| 590|       425|     0.99493|      100|\n",
      "|     device|  United States|  11|      82|      802|2015-11-25| 590|       425|     0.99493|      100|\n",
      "|     sensor|        Germany|  25|      50|      803|2015-11-25| 585|       424|     0.98649|       99|\n",
      "|      meter|  United States|  20|      96|      803|2015-11-25| 585|       424|     0.98649|      100|\n",
      "|     sensor|  United States|  34|      88|      803|2015-11-25| 585|       424|     0.98649|       99|\n",
      "|      meter|      Argentina|  33|      41|      803|2015-11-25| 585|       424|     0.98649|       99|\n",
      "|     sensor|  United States|  21|      87|      803|2015-11-25| 585|       424|     0.98649|       99|\n",
      "|     sensor|          Italy|  23|      62|      805|2015-11-25| 584|       423|      0.9848|       99|\n",
      "+-----------+---------------+----+--------+---------+----------+----+----------+------------+---------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Visualizando o efeito de ntile na borda de baixo\n",
    "df_c02_peak_ranking\\\n",
    "    .where(expr(\"date = '2015-11-25'\"))\\\n",
    "    .orderBy(\"c02_level\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções window: analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lag() e lead()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 260:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 17:25:19 WARN DAGScheduler: Broadcasting large task binary with size 1009.4 KiB\n",
      "+-------------+-------------------+----+--------------+----------+\n",
      "|      country|          timestamp|temp|preceding_temp|delta_temp|\n",
      "+-------------+-------------------+----+--------------+----------+\n",
      "|United States|2015-11-25 07:39:33|  25|          null|      null|\n",
      "|United States|2015-11-25 07:39:35|  33|            25|         8|\n",
      "|United States|2015-11-25 07:39:36|  16|            33|       -17|\n",
      "|United States|2015-11-25 07:39:38|  16|            16|         0|\n",
      "|United States|2015-11-25 07:54:13|  17|            16|         1|\n",
      "|United States|2015-11-25 07:54:15|  31|            17|        14|\n",
      "|United States|2015-11-25 07:54:16|  20|            31|       -11|\n",
      "|United States|2015-11-25 08:02:43|  30|            20|        10|\n",
      "|United States|2015-11-25 08:07:46|  14|            30|       -16|\n",
      "|United States|2015-11-25 08:22:04|  26|            14|        12|\n",
      "|United States|2015-11-25 08:22:06|  24|            10|        14|\n",
      "|United States|2015-11-25 08:22:06|  10|            26|       -16|\n",
      "|United States|2015-11-25 08:22:08|  23|            24|        -1|\n",
      "|United States|2015-11-25 08:22:09|  21|            23|        -2|\n",
      "|United States|2015-11-25 08:22:11|  14|            21|        -7|\n",
      "+-------------+-------------------+----+--------------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import lag\n",
    "\n",
    "# Definindo especificação de janela\n",
    "window_spec = Window\\\n",
    "    .partitionBy(\"country\")\\\n",
    "    .orderBy(\"timestamp\")\n",
    "\n",
    "# Obtendo temperaturas anteriores e posteriores para cálculo do delta\n",
    "df_delta_temp = df_iot.where(expr(\"country != ''\")).select(\n",
    "    \"country\",\n",
    "    \"timestamp\",\n",
    "    \"temp\",\n",
    "    lag(\"temp\", offset=1).over(window_spec).alias(\"preceding_temp\")\n",
    ").withColumn(\"delta_temp\", expr(\"temp - preceding_temp\"))\n",
    "\n",
    "# Visualizando resultado\n",
    "df_delta_temp\\\n",
    "    .where(expr(\"country = 'United States'\"))\\\n",
    "    .orderBy(\"timestamp\")\\\n",
    "    .show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 285:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 17:43:46 WARN DAGScheduler: Broadcasting large task binary with size 1019.7 KiB\n",
      "+-------------+-------------------+----+--------------+----------+---------------+\n",
      "|      country|          timestamp|temp|preceding_temp|delta_temp|rank_delta_temp|\n",
      "+-------------+-------------------+----+--------------+----------+---------------+\n",
      "|United States|2015-11-25 18:49:27|  34|            13|        21|              1|\n",
      "|United States|2015-11-25 23:41:12|  31|            10|        21|              1|\n",
      "|United States|2015-11-25 23:12:58|  33|            13|        20|              3|\n",
      "|United States|2015-11-25 12:48:48|  34|            15|        19|              4|\n",
      "|United States|2015-11-25 11:21:19|  29|            10|        19|              4|\n",
      "|United States|2015-11-25 22:09:26|  32|            13|        19|              4|\n",
      "|United States|2015-11-25 18:56:04|  29|            11|        18|              7|\n",
      "|United States|2015-11-25 17:15:25|  28|            10|        18|              7|\n",
      "|United States|2015-11-25 13:44:52|  28|            11|        17|              9|\n",
      "|United States|2015-11-25 16:36:33|  33|            16|        17|              9|\n",
      "|United States|2015-11-25 22:34:55|  33|            16|        17|              9|\n",
      "|United States|2015-11-25 11:57:22|  32|            15|        17|              9|\n",
      "|United States|2015-11-25 20:35:31|  29|            12|        17|              9|\n",
      "|United States|2015-11-25 17:47:59|  28|            12|        16|             14|\n",
      "|United States|2015-11-25 20:16:56|  28|            12|        16|             14|\n",
      "+-------------+-------------------+----+--------------+----------+---------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Especificando janela com deltas positivos recebendo as primeiras posições\n",
    "window_spec_ranking = Window\\\n",
    "    .partitionBy(\"country\", to_date(col(\"timestamp\")))\\\n",
    "    .orderBy(col(\"delta_temp\").desc())\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Criando nova coluna com base em ranking\n",
    "df_delta_temp_rank = df_delta_temp.withColumn(\"rank_delta_temp\", rank().over(window_spec_ranking))\\\n",
    "\n",
    "# Visualizando resultado\n",
    "df_delta_temp_rank\\\n",
    "    .where(expr(\"country = 'United States' and to_date(timestamp) = '2015-11-25'\"))\\\n",
    "    .orderBy(\"rank_delta_temp\")\\\n",
    "    .show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 297:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 17:51:48 WARN DAGScheduler: Broadcasting large task binary with size 1006.8 KiB\n",
      "+-------------+-------------------+----+--------------+--------------+\n",
      "|      country|          timestamp|temp|preceding_temp|following_temp|\n",
      "+-------------+-------------------+----+--------------+--------------+\n",
      "|United States|2015-11-25 07:39:33|  25|          null|            33|\n",
      "|United States|2015-11-25 07:39:35|  33|            25|            16|\n",
      "|United States|2015-11-25 07:39:36|  16|            33|            16|\n",
      "|United States|2015-11-25 07:39:38|  16|            16|            17|\n",
      "|United States|2015-11-25 07:54:13|  17|            16|            31|\n",
      "|United States|2015-11-25 07:54:15|  31|            17|            20|\n",
      "|United States|2015-11-25 07:54:16|  20|            31|            30|\n",
      "|United States|2015-11-25 08:02:43|  30|            20|            14|\n",
      "|United States|2015-11-25 08:07:46|  14|            30|            26|\n",
      "|United States|2015-11-25 08:22:04|  26|            14|            10|\n",
      "+-------------+-------------------+----+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import lag, lead\n",
    "\n",
    "# Visualizando temperaturas anteriores e posteriores\n",
    "df_delta_temp = df_iot.where(expr(\"country != ''\")).select(\n",
    "    \"country\",\n",
    "    \"timestamp\",\n",
    "    \"temp\",\n",
    "    lag(\"temp\", offset=1).over(window_spec).alias(\"preceding_temp\"),\n",
    "    lead(\"temp\", offset=1).over(window_spec).alias(\"following_temp\")\n",
    ")\n",
    "\n",
    "# Visualizando resultado\n",
    "df_delta_temp\\\n",
    "    .where(expr(\"country = 'United States'\"))\\\n",
    "    .orderBy(\"timestamp\")\\\n",
    "    .show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções window: agregações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 303:==========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 18:12:34 WARN DAGScheduler: Broadcasting large task binary with size 1008.2 KiB\n",
      "+-------------+----------+-------------------+---------+--------------+\n",
      "|      country|      date|          timestamp|c02_level|cumulative_c02|\n",
      "+-------------+----------+-------------------+---------+--------------+\n",
      "|United States|2015-11-25|2015-11-25 07:39:33|     1566|          1566|\n",
      "|United States|2015-11-25|2015-11-25 07:39:35|      965|          2531|\n",
      "|United States|2015-11-25|2015-11-25 07:39:36|     1385|          3916|\n",
      "|United States|2015-11-25|2015-11-25 07:39:38|     1047|          4963|\n",
      "|United States|2015-11-25|2015-11-25 07:54:13|      816|          5779|\n",
      "|United States|2015-11-25|2015-11-25 07:54:15|      834|          6613|\n",
      "|United States|2015-11-25|2015-11-25 07:54:16|     1184|          7797|\n",
      "|United States|2015-11-25|2015-11-25 08:02:43|      967|          8764|\n",
      "|United States|2015-11-25|2015-11-25 08:07:46|     1434|         10198|\n",
      "|United States|2015-11-25|2015-11-25 08:22:04|     1287|         11485|\n",
      "|United States|2015-11-25|2015-11-25 08:22:06|     1431|         14472|\n",
      "|United States|2015-11-25|2015-11-25 08:22:06|     1556|         13041|\n",
      "|United States|2015-11-25|2015-11-25 08:22:08|      835|         15307|\n",
      "|United States|2015-11-25|2015-11-25 08:22:09|     1155|         16462|\n",
      "|United States|2015-11-25|2015-11-25 08:22:11|      851|         17313|\n",
      "+-------------+----------+-------------------+---------+--------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Defindo janela\n",
    "window_spec = Window\\\n",
    "    .partitionBy(\"country\", \"date\")\\\n",
    "    .orderBy(\"timestamp\")\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "# Retornando soma acumulada de emissão de gás carbônico por dia\n",
    "df_c02_cumulative = df_iot.select(\n",
    "    \"country\",\n",
    "    \"date\",\n",
    "    \"timestamp\",\n",
    "    \"c02_level\",\n",
    "    sum(\"c02_level\").over(window_spec).alias(\"cumulative_c02\")\n",
    ")\n",
    "\n",
    "# Visualizando resultado\n",
    "df_c02_cumulative\\\n",
    "    .where(expr(\"country = 'United States'\"))\\\n",
    "    .orderBy(\"timestamp\")\\\n",
    "    .show(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Média Móvel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 321:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 18:37:55 WARN DAGScheduler: Broadcasting large task binary with size 1011.6 KiB\n",
      "+-------------+----------+-------------------+----+----------------+\n",
      "|      country|      date|          timestamp|temp|rolling_avg_temp|\n",
      "+-------------+----------+-------------------+----+----------------+\n",
      "|United States|2015-11-25|2015-11-25 07:39:33|  25|            25.0|\n",
      "|United States|2015-11-25|2015-11-25 07:39:35|  33|            29.0|\n",
      "|United States|2015-11-25|2015-11-25 07:39:36|  16|           24.67|\n",
      "|United States|2015-11-25|2015-11-25 07:39:38|  16|           21.67|\n",
      "|United States|2015-11-25|2015-11-25 07:54:13|  17|           16.33|\n",
      "|United States|2015-11-25|2015-11-25 07:54:15|  31|           21.33|\n",
      "|United States|2015-11-25|2015-11-25 07:54:16|  20|           22.67|\n",
      "|United States|2015-11-25|2015-11-25 08:02:43|  30|            27.0|\n",
      "|United States|2015-11-25|2015-11-25 08:07:46|  14|           21.33|\n",
      "|United States|2015-11-25|2015-11-25 08:22:04|  26|           23.33|\n",
      "|United States|2015-11-25|2015-11-25 08:22:06|  24|            20.0|\n",
      "|United States|2015-11-25|2015-11-25 08:22:06|  10|           16.67|\n",
      "|United States|2015-11-25|2015-11-25 08:22:08|  23|            19.0|\n",
      "|United States|2015-11-25|2015-11-25 08:22:09|  21|           22.67|\n",
      "|United States|2015-11-25|2015-11-25 08:22:11|  14|           19.33|\n",
      "+-------------+----------+-------------------+----+----------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "# Definindo especificação de janela\n",
    "window_spec_avg = Window\\\n",
    "    .partitionBy(\"country\", \"date\")\\\n",
    "    .orderBy(\"timestamp\")\\\n",
    "    .rowsBetween(-2, Window.currentRow)\n",
    "\n",
    "# Retornando a média móvel de temperatura\n",
    "df_rolling_avg_temp = df_iot.select(\n",
    "    \"country\",\n",
    "    \"date\",\n",
    "    \"timestamp\",\n",
    "    \"temp\",\n",
    "    round(avg(\"temp\").over(window_spec_avg), 2).alias(\"rolling_avg_temp\")\n",
    ")\n",
    "\n",
    "# Visualizando resultado\n",
    "df_rolling_avg_temp\\\n",
    "    .where(expr(\"country = 'United States'\"))\\\n",
    "    .orderBy(\"timestamp\")\\\n",
    "    .show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 327:============================>                            (2 + 2) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/07 18:42:18 WARN DAGScheduler: Broadcasting large task binary with size 1017.2 KiB\n",
      "+-------------+----------+-------------------+----+-------+-------+------------------+------------------+\n",
      "|      country|      date|          timestamp|temp|temp_t1|temp_t2|window_rolling_avg|manual_rolling_avg|\n",
      "+-------------+----------+-------------------+----+-------+-------+------------------+------------------+\n",
      "|United States|2015-11-25|2015-11-25 07:39:33|  25|   null|   null|              25.0|              25.0|\n",
      "|United States|2015-11-25|2015-11-25 07:39:35|  33|     25|   null|              29.0|              29.0|\n",
      "|United States|2015-11-25|2015-11-25 07:39:36|  16|     33|     25|             24.67|             24.67|\n",
      "|United States|2015-11-25|2015-11-25 07:39:38|  16|     16|     33|             21.67|             21.67|\n",
      "|United States|2015-11-25|2015-11-25 07:54:13|  17|     16|     16|             16.33|             16.33|\n",
      "|United States|2015-11-25|2015-11-25 07:54:15|  31|     17|     16|             21.33|             21.33|\n",
      "|United States|2015-11-25|2015-11-25 07:54:16|  20|     31|     17|             22.67|             22.67|\n",
      "|United States|2015-11-25|2015-11-25 08:02:43|  30|     20|     31|              27.0|              27.0|\n",
      "|United States|2015-11-25|2015-11-25 08:07:46|  14|     30|     20|             21.33|             21.33|\n",
      "|United States|2015-11-25|2015-11-25 08:22:04|  26|     14|     30|             23.33|             23.33|\n",
      "|United States|2015-11-25|2015-11-25 08:22:06|  24|     10|     26|              20.0|              20.0|\n",
      "|United States|2015-11-25|2015-11-25 08:22:06|  10|     26|     14|             16.67|             16.67|\n",
      "|United States|2015-11-25|2015-11-25 08:22:08|  23|     24|     10|              19.0|              19.0|\n",
      "|United States|2015-11-25|2015-11-25 08:22:09|  21|     23|     24|             22.67|             22.67|\n",
      "|United States|2015-11-25|2015-11-25 08:22:11|  14|     21|     23|             19.33|             19.33|\n",
      "+-------------+----------+-------------------+----+-------+-------+------------------+------------------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Definindo especificação de janela para função lag\n",
    "window_spec_lag = Window\\\n",
    "    .partitionBy(\"country\", \"date\")\\\n",
    "    .orderBy(\"timestamp\")\n",
    "\n",
    "# Criando média móvel na mão via lag\n",
    "df_avg_lag = df_iot.select(\n",
    "    \"country\",\n",
    "    \"date\",\n",
    "    \"timestamp\",\n",
    "    \"temp\",\n",
    "    lag(\"temp\", offset=1).over(window_spec_lag).alias(\"temp_t1\"),\n",
    "    lag(\"temp\", offset=2).over(window_spec_lag).alias(\"temp_t2\"),\n",
    "    round(avg(\"temp\").over(window_spec_avg), 2).alias(\"window_rolling_avg\")\n",
    ").selectExpr(\n",
    "    \"*\",\n",
    "    \"case when temp_t1 is null and temp_t2 is null then temp \\\n",
    "        when temp_t1 is not null and temp_t2 is null then round((temp + temp_t1) / 2, 2) \\\n",
    "        else round((temp + temp_t1 + temp_t2) / 3, 2) \\\n",
    "    end as manual_rolling_avg\"\n",
    ")\n",
    "\n",
    "# Visualizando resultado\n",
    "df_avg_lag\\\n",
    "    .where(expr(\"country = 'United States'\"))\\\n",
    "    .orderBy(\"timestamp\")\\\n",
    "    .show(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "cannot resolve 'RANGE BETWEEN CAST(-604800L AS STRING) FOLLOWING AND CURRENT ROW' due to data type mismatch: The data type of the lower bound 'string' does not match the expected data type '(numeric or interval day to second or interval year to month or interval)'.;\n'Project [country#944, date#970, timestamp#961, temp#9, avg(temp#9) windowspecdefinition(country#944, date#970, timestamp#961 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-604800 as string), currentrow$())) AS rolling_avg_temp#5292]\n+- Project [device_type#943, country#944, temp#9, humidity#10, c02_level#12, timestamp#961, date#970]\n   +- Project [device_type#943, country#944, temp#9, humidity#10, c02_level#12, timestamp_raw#945L, date_add#953, timestamp#961, to_date(timestamp#961, None, Some(America/Sao_Paulo)) AS date#970]\n      +- Project [device_type#943, country#944, temp#9, humidity#10, c02_level#12, timestamp_raw#945L, date_add#953, from_unixtime(cast(((cast(timestamp_raw#945L as double) / cast(1000 as double)) + cast(date_add#953 as double)) as bigint), yyyy-MM-dd HH:mm:ss, Some(America/Sao_Paulo)) AS timestamp#961]\n         +- Project [device_type#943, country#944, temp#9, humidity#10, c02_level#12, timestamp_raw#945L, <lambda>(monotonically_increasing_id())#952 AS date_add#953]\n            +- Project [split(device_name#1, -, -1)[0] AS device_type#943, cn#5 AS country#944, temp#9, humidity#10, c02_level#12, timestamp#14L AS timestamp_raw#945L]\n               +- Relation [device_id#0,device_name#1,ip#2,cca2#3,cca3#4,cn#5,latitude#6,longitude#7,scale#8,temp#9,humidity#10,battery_level#11,c02_level#12,lcd#13,timestamp#14L] json\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb Cell 41\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=3'>4</a>\u001b[0m window_spec_avg_timeseries \u001b[39m=\u001b[39m Window\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=4'>5</a>\u001b[0m     \u001b[39m.\u001b[39mpartitionBy(\u001b[39m\"\u001b[39m\u001b[39mcountry\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mdate\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=5'>6</a>\u001b[0m     \u001b[39m.\u001b[39morderBy(\u001b[39m\"\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m\"\u001b[39m)\\\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=6'>7</a>\u001b[0m     \u001b[39m.\u001b[39mrangeBetween(\u001b[39m-\u001b[39mdays(\u001b[39m7\u001b[39m), \u001b[39m0\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=8'>9</a>\u001b[0m \u001b[39m# Aplicando média móvel em janela temporal\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=9'>10</a>\u001b[0m df_rolling_avg_temp_dynamic \u001b[39m=\u001b[39m df_iot\u001b[39m.\u001b[39;49mselect(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=10'>11</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mcountry\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=11'>12</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mdate\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=12'>13</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtimestamp\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=13'>14</a>\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mtemp\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=14'>15</a>\u001b[0m     avg(\u001b[39m\"\u001b[39;49m\u001b[39mtemp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mover(window_spec_avg_timeseries)\u001b[39m.\u001b[39;49malias(\u001b[39m\"\u001b[39;49m\u001b[39mrolling_avg_temp\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art18-window-functions.ipynb#ch0000063?line=15'>16</a>\u001b[0m )\n",
      "File \u001b[0;32m~/dev/python/venvs/pyspark-venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2023\u001b[0m, in \u001b[0;36mDataFrame.select\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m   2002\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mselect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcols: \u001b[39m\"\u001b[39m\u001b[39mColumnOrName\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m:  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m     \u001b[39m\"\"\"Projects a set of expressions and returns a new :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \n\u001b[1;32m   2005\u001b[0m \u001b[39m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2021\u001b[0m \u001b[39m    [Row(name='Alice', age=12), Row(name='Bob', age=15)]\u001b[39;00m\n\u001b[1;32m   2022\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2023\u001b[0m     jdf \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mselect(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jcols(\u001b[39m*\u001b[39;49mcols))\n\u001b[1;32m   2024\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(jdf, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/dev/python/venvs/pyspark-venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/dev/python/venvs/pyspark-venv/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: cannot resolve 'RANGE BETWEEN CAST(-604800L AS STRING) FOLLOWING AND CURRENT ROW' due to data type mismatch: The data type of the lower bound 'string' does not match the expected data type '(numeric or interval day to second or interval year to month or interval)'.;\n'Project [country#944, date#970, timestamp#961, temp#9, avg(temp#9) windowspecdefinition(country#944, date#970, timestamp#961 ASC NULLS FIRST, specifiedwindowframe(RangeFrame, cast(-604800 as string), currentrow$())) AS rolling_avg_temp#5292]\n+- Project [device_type#943, country#944, temp#9, humidity#10, c02_level#12, timestamp#961, date#970]\n   +- Project [device_type#943, country#944, temp#9, humidity#10, c02_level#12, timestamp_raw#945L, date_add#953, timestamp#961, to_date(timestamp#961, None, Some(America/Sao_Paulo)) AS date#970]\n      +- Project [device_type#943, country#944, temp#9, humidity#10, c02_level#12, timestamp_raw#945L, date_add#953, from_unixtime(cast(((cast(timestamp_raw#945L as double) / cast(1000 as double)) + cast(date_add#953 as double)) as bigint), yyyy-MM-dd HH:mm:ss, Some(America/Sao_Paulo)) AS timestamp#961]\n         +- Project [device_type#943, country#944, temp#9, humidity#10, c02_level#12, timestamp_raw#945L, <lambda>(monotonically_increasing_id())#952 AS date_add#953]\n            +- Project [split(device_name#1, -, -1)[0] AS device_type#943, cn#5 AS country#944, temp#9, humidity#10, c02_level#12, timestamp#14L AS timestamp_raw#945L]\n               +- Relation [device_id#0,device_name#1,ip#2,cca2#3,cca3#4,cn#5,latitude#6,longitude#7,scale#8,temp#9,humidity#10,battery_level#11,c02_level#12,lcd#13,timestamp#14L] json\n"
     ]
    }
   ],
   "source": [
    "days = lambda i: i * 86400 \n",
    "\n",
    "# Especificando janela para média móvel dos últimos 7 dias\n",
    "window_spec_avg_timeseries = Window\\\n",
    "    .partitionBy(\"country\", \"date\")\\\n",
    "    .orderBy(\"timestamp\")\\\n",
    "    .rangeBetween(-days(7), 0)\n",
    "\n",
    "# Aplicando média móvel em janela temporal\n",
    "df_rolling_avg_temp_dynamic = df_iot.select(\n",
    "    \"country\",\n",
    "    \"date\",\n",
    "    \"timestamp\",\n",
    "    \"temp\",\n",
    "    avg(\"temp\").over(window_spec_avg_timeseries).alias(\"rolling_avg_temp\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuntos:\n",
    "\n",
    "- Teoria sobre funções *window*\n",
    "- Obtenção e preparação dos dados\n",
    "- Exemplo inicial de agregações e funções window\n",
    "- Etapas para aplicação de funções window (definição de especificação, etc)\n",
    "    - Exemplos práticos de função window\n",
    "    - Funções de ranking\n",
    "        - rank() e dense_rank()\n",
    "        - row_number()\n",
    "        - percent_rank(),\n",
    "        - ntile()\n",
    "    - Funções analíticas\n",
    "        - lag()\n",
    "        - lead()\n",
    "    - Funções de agregação\n",
    "        - exemplos práticos (média, máximo, soma cumulativa, etc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyspark-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3a59d43698a1c21b8ea695e9dada7fb9edd9002f05e8c3363eca03517868e40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

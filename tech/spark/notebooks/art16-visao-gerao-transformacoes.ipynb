{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 14:27:43 WARN Utils: Your hostname, panini-ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.0.110 instead (on interface enp3s0)\n",
      "22/09/19 14:27:43 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/09/19 14:27:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Construindo objeto de sessão\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"transformacoes-pyspark\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definindo variáveis de diretório\n",
    "home_path = os.path.expanduser(\"~\")\n",
    "data_path = os.path.join(home_path, \"dev/panini-tech-lab/data/brazilian-ecommerce\")\n",
    "\n",
    "# Realizando leitura dos dados: orders\n",
    "df_orders = spark.read.format(\"csv\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(os.path.join(data_path, \"orders/\"))\n",
    "\n",
    "# Realizando leitura dos dados: order items\n",
    "df_order_items = spark.read.format(\"csv\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(os.path.join(data_path, \"order_items/\"))\n",
    "\n",
    "# Realizando leitura dos dados: order payments\n",
    "df_order_payments = spark.read.format(\"csv\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(os.path.join(data_path, \"order_payments/\"))\n",
    "\n",
    "# Realizando leitura dos dados: order reviews\n",
    "df_order_reviews = spark.read.format(\"csv\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(os.path.join(data_path, \"order_reviews/\"))\n",
    "\n",
    "# Realizando leitura dos dados: products\n",
    "df_products = spark.read.format(\"csv\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(os.path.join(data_path, \"products/\"))\n",
    "\n",
    "# Realizando leitura dos dados: customers\n",
    "df_customers = spark.read.format(\"csv\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(os.path.join(data_path, \"customers/\"))\n",
    "\n",
    "# Realizando leitura dos dados: geolocation\n",
    "df_geolocation = spark.read.format(\"csv\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(os.path.join(data_path, \"geolocation/\"))\n",
    "\n",
    "# Realizando leitura dos dados: sellers\n",
    "df_sellers = spark.read.format(\"csv\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(os.path.join(data_path, \"sellers/\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <br><img src=\"https://i.imgur.com/HRhd2Y0.png\" width=750 height=450 alt=\"brazilian e-commerce star schema\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+------------------------+---------------------+--------------+\n",
      "|customer_id                     |customer_unique_id              |customer_zip_code_prefix|customer_city        |customer_state|\n",
      "+--------------------------------+--------------------------------+------------------------+---------------------+--------------+\n",
      "|06b8999e2fba1a1fbc88172c00ba8bc7|861eff4711a542e4b93843c6dd7febb0|14409                   |franca               |SP            |\n",
      "|18955e83d337fd6b2def6b18a428ac77|290c77bc529b7ac935b93aa66c333dc3|9790                    |sao bernardo do campo|SP            |\n",
      "|4e7b3e00288586ebd08712fdd0374a03|060e732b5b29e8181a18229c7b0b2b5e|1151                    |sao paulo            |SP            |\n",
      "|b2b6027bc5c5109e529d4dc6358b12c3|259dac757896d24d7702b9acbbff3f3c|8775                    |mogi das cruzes      |SP            |\n",
      "|4f2d8ab171c80ec8364f7c12e35b23ad|345ecd01c38d18a9036ed96c73b8d066|13056                   |campinas             |SP            |\n",
      "+--------------------------------+--------------------------------+------------------------+---------------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando tipos primitivos\n",
    "from pyspark.sql.types import StructType, StructField, \\\n",
    "    StringType, IntegerType\n",
    "\n",
    "# Definindo schema\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), nullable=True, metadata={\"description\": \"Id do cliente\"}),\n",
    "    StructField(\"customer_unique_id\", StringType(), nullable=True, metadata={\"description\": \"Id único do cliente\"}),\n",
    "    StructField(\"customer_zip_code_prefix\", IntegerType(), nullable=True, metadata={\"description\": \"Prefixo do CEP do cliente\"}),\n",
    "    StructField(\"customer_city\", StringType(), nullable=True, metadata={\"description\": \"Cidade do cliente\"}),\n",
    "    StructField(\"customer_state\", StringType(), nullable=True, metadata={\"description\": \"Estado do cliente\"})\n",
    "])\n",
    "\n",
    "# Realizando a leitura dos dados\n",
    "df_customers = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .schema(customers_schema)\\\n",
    "    .load(os.path.join(data_path, \"customers/\"))\n",
    "\n",
    "# Verificando amostra dos dados\n",
    "df_customers.printSchema()\n",
    "df_customers.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando Consultas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### col()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'split(nome_completo,  , -1)'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Referenciando colunas e criando expressões\n",
    "col(\"anomes\") * 100 + 1\n",
    "col(\"valor_A\") > col(\"valor_B\")\n",
    "\n",
    "# Aplicando funções à referências de colunas\n",
    "from pyspark.sql.functions import lpad, lower, split\n",
    "\n",
    "lpad(col(\"moeda\"), 2, \"\")\n",
    "lower(col(\"sigla\"))\n",
    "split(\"nome_completo\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'split(nome_completo, )'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "# Criando expressões em formato de string\n",
    "expr(\"(anomes * 100) + 1\")\n",
    "expr(\"valor_A > valor_B\")\n",
    "expr(\"lpad(moeda, 2, '')\")\n",
    "expr(\"lower(sigla)\")\n",
    "expr(\"split(nome_completo, '')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### select()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+----------+-------------+-----------------+\n",
      "|           id_pedido| dt_compra|dt_aprovacao|dt_entrega|status_pedido|dias_para_entrega|\n",
      "+--------------------+----------+------------+----------+-------------+-----------------+\n",
      "|e481f51cbdc54678b...|2017-10-02|  2017-10-02|2017-10-10|    DELIVERED|                8|\n",
      "|53cdb2fc8bc7dce0b...|2018-07-24|  2018-07-26|2018-08-07|    DELIVERED|               14|\n",
      "|47770eb9100c2d0c4...|2018-08-08|  2018-08-08|2018-08-17|    DELIVERED|                9|\n",
      "|949d5b44dbf5de918...|2017-11-18|  2017-11-18|2017-12-02|    DELIVERED|               14|\n",
      "|ad21c59c0840e6cb8...|2018-02-13|  2018-02-13|2018-02-16|    DELIVERED|                3|\n",
      "+--------------------+----------+------------+----------+-------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import upper, datediff\n",
    "\n",
    "# Verificando detalhes sobre o pedido\n",
    "df_orders_prep = df_orders.select(\n",
    "    expr(\"order_id AS id_pedido\"),\n",
    "    expr(\"to_date(order_purchase_timestamp) AS dt_compra\"),\n",
    "    expr(\"to_date(order_approved_at) AS dt_aprovacao\"),\n",
    "    expr(\"to_date(order_delivered_customer_date) AS dt_entrega\"),\n",
    "    upper(col(\"order_status\")).alias(\"status_pedido\"),\n",
    "    datediff(col(\"order_delivered_customer_date\"), col(\"order_purchase_timestamp\")).alias(\"dias_para_entrega\"),\n",
    ")\n",
    "\n",
    "# Visualizando dados\n",
    "df_orders_prep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### selectExpr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------+----------+-------------+-----------------+-------------+\n",
      "|           id_pedido| dt_compra|dt_aprovacao|dt_entrega|status_pedido|dias_para_entrega|flag_entregue|\n",
      "+--------------------+----------+------------+----------+-------------+-----------------+-------------+\n",
      "|e481f51cbdc54678b...|2017-10-02|  2017-10-02|2017-10-10|    DELIVERED|                8|            1|\n",
      "|53cdb2fc8bc7dce0b...|2018-07-24|  2018-07-26|2018-08-07|    DELIVERED|               14|            1|\n",
      "|47770eb9100c2d0c4...|2018-08-08|  2018-08-08|2018-08-17|    DELIVERED|                9|            1|\n",
      "|949d5b44dbf5de918...|2017-11-18|  2017-11-18|2017-12-02|    DELIVERED|               14|            1|\n",
      "|ad21c59c0840e6cb8...|2018-02-13|  2018-02-13|2018-02-16|    DELIVERED|                3|            1|\n",
      "+--------------------+----------+------------+----------+-------------+-----------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verificando detalhes sobre o pedido\n",
    "df_orders_prep = df_orders.selectExpr(\n",
    "    \"order_id AS id_pedido\",\n",
    "    \"to_date(order_purchase_timestamp) AS dt_compra\",\n",
    "    \"to_date(order_approved_at) AS dt_aprovacao\",\n",
    "    \"to_date(order_delivered_customer_date) AS dt_entrega\",\n",
    "    \"upper(order_status) AS status_pedido\",\n",
    "    \"datediff(order_delivered_customer_date, order_purchase_timestamp) AS dias_para_entrega\",\n",
    "    \"case when upper(order_status) = 'DELIVERED' then 1 else 0 end as flag_entregue\"\n",
    ")\n",
    "\n",
    "# Visualizando dados\n",
    "df_orders_prep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operações em colunas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+------------+--------------------+-------------+-----+--------------+--------------+\n",
      "|            order_id|payment_sequential|payment_type|payment_installments|payment_value|moeda|vlr_pgto_moeda|tipo_pagamento|\n",
      "+--------------------+------------------+------------+--------------------+-------------+-----+--------------+--------------+\n",
      "|b81ef226f3fe1789b...|                 1| credit_card|                   8|        99.33|   R$|       R$99.33|        credit|\n",
      "|a9810da82917af2d9...|                 1| credit_card|                   1|        24.39|   R$|       R$24.39|        credit|\n",
      "|25e8ea4e93396b6fa...|                 1| credit_card|                   1|        65.71|   R$|       R$65.71|        credit|\n",
      "|ba78997921bbcdc13...|                 1| credit_card|                   8|       107.78|   R$|      R$107.78|        credit|\n",
      "|42fdf880ba16b47b5...|                 1| credit_card|                   2|       128.45|   R$|      R$128.45|        credit|\n",
      "+--------------------+------------------+------------+--------------------+-------------+-----+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Adicionando colunas em um DataFrame\n",
    "df_payments_prep = df_order_payments\\\n",
    "    .withColumn(\"moeda\", expr(\"'R$'\"))\\\n",
    "    .withColumn(\"vlr_pgto_moeda\", expr(\"concat(moeda, cast(payment_value AS string))\"))\\\n",
    "    .withColumn(\"tipo_pagamento\", split(col(\"payment_type\"), \"_\")[0])\n",
    "\n",
    "# Visualizando dados\n",
    "df_payments_prep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### withColumnRenamed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----------+------------+--------+-----+\n",
      "|           id_pedido|parcela_pgto|  tipo_pgto|qtd_parcelas|vlr_pgto|moeda|\n",
      "+--------------------+------------+-----------+------------+--------+-----+\n",
      "|b81ef226f3fe1789b...|           1|credit_card|           8|   99.33|   R$|\n",
      "|a9810da82917af2d9...|           1|credit_card|           1|   24.39|   R$|\n",
      "|25e8ea4e93396b6fa...|           1|credit_card|           1|   65.71|   R$|\n",
      "|ba78997921bbcdc13...|           1|credit_card|           8|  107.78|   R$|\n",
      "|42fdf880ba16b47b5...|           1|credit_card|           2|  128.45|   R$|\n",
      "+--------------------+------------+-----------+------------+--------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tratando base de pagamentos através de métodos característicos\n",
    "df_payments_prep = df_order_payments\\\n",
    "    .withColumnRenamed(\"order_id\", \"id_pedido\")\\\n",
    "    .withColumnRenamed(\"payment_sequential\", \"parcela_pgto\")\\\n",
    "    .withColumnRenamed(\"payment_type\", \"tipo_pgto\")\\\n",
    "    .withColumnRenamed(\"payment_installments\", \"qtd_parcelas\")\\\n",
    "    .withColumnRenamed(\"payment_value\", \"vlr_pgto\")\\\n",
    "    .withColumn(\"moeda\", expr(\"'R$'\"))\n",
    "\n",
    "# Visualizando resultado\n",
    "df_payments_prep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### alias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+-----------------+---------------+\n",
      "|id_vendedor                     |cidade_vendedor  |estado_vendedor|\n",
      "+--------------------------------+-----------------+---------------+\n",
      "|3442f8959a84dea7ee197c632cb2df15|campinas         |SP             |\n",
      "|d1b65fc7debc3361ea86b5f14c68d2e2|mogi guacu       |SP             |\n",
      "|ce3ad9de960102d0677a81f5d0bb7b2d|rio de janeiro   |RJ             |\n",
      "|c0f3eea2e14555b6faeea3dd58c1b1c3|sao paulo        |SP             |\n",
      "|51a04a8a6bdcb23deccc82b0b80742cf|braganca paulista|SP             |\n",
      "+--------------------------------+-----------------+---------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando consulta e adicionando alias \n",
    "df_sellers_prep = df_sellers.select(\n",
    "    col(\"seller_id\").alias(\"id_vendedor\"),\n",
    "    col(\"seller_city\").alias(\"cidade_vendedor\"),\n",
    "    col(\"seller_state\").alias(\"estado_vendedor\")\n",
    ")\n",
    "\n",
    "# Visualizando dados\n",
    "df_sellers_prep.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|   geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+-------------------+------------------+----------------+-----------------+\n",
      "|                       1037| -23.54562128115268|-46.63929204800168|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535|-46.64482029837157|       sao paulo|               SP|\n",
      "|                       1046| -23.54612896641469|-46.64295148361138|       sao paulo|               SP|\n",
      "|                       1041|  -23.5443921648681|-46.63949930627844|       sao paulo|               SP|\n",
      "|                       1035|-23.541577961711493|-46.64160722329613|       sao paulo|               SP|\n",
      "+---------------------------+-------------------+------------------+----------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------------------------+-------------------+------------------+----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|   geolocation_lng|geolocation_city|\n",
      "+---------------------------+-------------------+------------------+----------------+\n",
      "|                       1037| -23.54562128115268|-46.63929204800168|       sao paulo|\n",
      "|                       1046|-23.546081127035535|-46.64482029837157|       sao paulo|\n",
      "|                       1046| -23.54612896641469|-46.64295148361138|       sao paulo|\n",
      "|                       1041|  -23.5443921648681|-46.63949930627844|       sao paulo|\n",
      "|                       1035|-23.541577961711493|-46.64160722329613|       sao paulo|\n",
      "+---------------------------+-------------------+------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# DataFrame original de geolocalização\n",
    "df_geolocation.show(5)\n",
    "\n",
    "# Eliminando colunas de cidade\n",
    "df_geo_dropped = df_geolocation.drop(\"geolocation_state\")\n",
    "df_geo_dropped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+------------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|   geolocation_lng|\n",
      "+---------------------------+-------------------+------------------+\n",
      "|                       1037| -23.54562128115268|-46.63929204800168|\n",
      "|                       1046|-23.546081127035535|-46.64482029837157|\n",
      "|                       1046| -23.54612896641469|-46.64295148361138|\n",
      "|                       1041|  -23.5443921648681|-46.63949930627844|\n",
      "|                       1035|-23.541577961711493|-46.64160722329613|\n",
      "+---------------------------+-------------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Eliminando múltiplas colunas\n",
    "df_geolocation.drop(\"geolocation_city\", \"geolocation_state\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----------+------------+------------------+--------+\n",
      "|           id_pedido|flag_pgto_realizado|  tipo_pgto|qtd_parcelas|flag_pgto_maior_3x|vlr_pgto|\n",
      "+--------------------+-------------------+-----------+------------+------------------+--------+\n",
      "|b81ef226f3fe1789b...|                  1|credit_card|           8|                 1|   99.33|\n",
      "|a9810da82917af2d9...|                  1|credit_card|           1|                 0|   24.39|\n",
      "|25e8ea4e93396b6fa...|                  1|credit_card|           1|                 0|   65.71|\n",
      "|ba78997921bbcdc13...|                  1|credit_card|           8|                 1|  107.78|\n",
      "|42fdf880ba16b47b5...|                  1|credit_card|           2|                 0|  128.45|\n",
      "+--------------------+-------------------+-----------+------------+------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import lit, when\n",
    "\n",
    "# Analisando pagamentos online\n",
    "df_pgtos = df_order_payments.select(\n",
    "    col(\"order_id\").alias(\"id_pedido\"),\n",
    "    lit(1).alias(\"flag_pgto_realizado\"),\n",
    "    col(\"payment_type\").alias(\"tipo_pgto\"),\n",
    "    col(\"payment_installments\").alias(\"qtd_parcelas\"),\n",
    "    when(col(\"payment_installments\") >= 3, lit(1)).otherwise(lit(0)).alias(\"flag_pgto_maior_3x\"),\n",
    "    col(\"payment_value\").alias(\"vlr_pgto\")\n",
    ")\n",
    "\n",
    "# Visualizando dados\n",
    "df_pgtos.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+----------+----------+----------+-------------+----------------+\n",
      "|           id_pedido| dt_compra|ano_compra|mes_compra|dia_compra|anomes_compra|anomesdia_compra|\n",
      "+--------------------+----------+----------+----------+----------+-------------+----------------+\n",
      "|e481f51cbdc54678b...|2017-10-02|      2017|        10|         2|       201710|        20175602|\n",
      "|53cdb2fc8bc7dce0b...|2018-07-24|      2018|         7|        24|       201807|        20184124|\n",
      "|47770eb9100c2d0c4...|2018-08-08|      2018|         8|         8|       201808|        20183808|\n",
      "|949d5b44dbf5de918...|2017-11-18|      2017|        11|        18|       201711|        20172818|\n",
      "|ad21c59c0840e6cb8...|2018-02-13|      2018|         2|        13|       201802|        20181813|\n",
      "+--------------------+----------+----------+----------+----------+-------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função e tipos primitivos\n",
    "from pyspark.sql.functions import cast, year, month, dayofmonth\n",
    "from pyspark.sql.types import DateType, IntegerType\n",
    "\n",
    "# Referenciando expressões de transformação\n",
    "ano_compra = year(col(\"order_purchase_timestamp\").cast(DateType()))\n",
    "mes_compra = month(col(\"order_purchase_timestamp\").cast(DateType()))\n",
    "dia_compra = dayofmonth(col(\"order_purchase_timestamp\").cast(DateType()))\n",
    "\n",
    "# Transformando datas de base de pedidos\n",
    "df_orders_prep = df_orders.select(\n",
    "    col(\"order_id\").alias(\"id_pedido\"),\n",
    "    col(\"order_purchase_timestamp\").cast(DateType()).alias(\"dt_compra\"),\n",
    "    ano_compra.alias(\"ano_compra\"),\n",
    "    mes_compra.alias(\"mes_compra\"),\n",
    "    dia_compra.alias(\"dia_compra\"),\n",
    "    ((ano_compra * 100) + mes_compra).cast(IntegerType()).alias(\"anomes_compra\"),\n",
    "    expr(\"cast(date_format(order_purchase_timestamp, 'yyyymmdd') AS INT) AS anomesdia_compra\")\n",
    ")\n",
    "\n",
    "# Visualizando dados\n",
    "df_orders_prep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Operações em Registros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### where()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------+--------------+\n",
      "|id_cliente                      |cidade_cliente|estado_cliente|\n",
      "+--------------------------------+--------------+--------------+\n",
      "|4f2d8ab171c80ec8364f7c12e35b23ad|campinas      |SP            |\n",
      "|4ad4f929392158a3bb76b3ec02a751b2|campinas      |SP            |\n",
      "|ccb8e120e8af0bbf5a1daa2f21984d7b|campinas      |SP            |\n",
      "|fb04e5eb553d32e040a1a83cf436a65c|campinas      |SP            |\n",
      "|5cb2307ae35b44c9dd90f76c649b99b5|campinas      |SP            |\n",
      "+--------------------------------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrando clientes de uma cidade em específico\n",
    "city_filter = col(\"customer_city\") == \"campinas\"\n",
    "\n",
    "# Criando consulta\n",
    "df_customers_sbc = df_customers.selectExpr(\n",
    "    \"customer_id AS id_cliente\",\n",
    "    \"customer_city AS cidade_cliente\",\n",
    "    \"customer_state AS estado_cliente\"\n",
    ").where(city_filter)\n",
    "#.where(expr(\"customer_city = 'campinas'\"))\n",
    "\n",
    "# Visualizando resultado\n",
    "df_customers_sbc.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort() e orderBy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 28:==============>                                           (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------+\n",
      "|          id_produto|vlr_produto|vlr_frete|\n",
      "+--------------------+-----------+---------+\n",
      "|489ae2aa008f02150...|     6735.0|   194.31|\n",
      "|69c590f7ffc7bf8db...|     6729.0|   193.21|\n",
      "|1bdf5e6731585cf01...|     6499.0|   227.66|\n",
      "|a6492cc69376c469a...|     4799.0|   151.34|\n",
      "|c3ed642d592594bb6...|     4690.0|    74.34|\n",
      "+--------------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Retornando itens mais caros\n",
    "df_expensive_items = df_order_items.selectExpr(\n",
    "    \"product_id AS id_produto\",\n",
    "    \"price AS vlr_produto\",\n",
    "    \"freight_value AS vlr_frete\"\n",
    ").sort([\"price\", \"freight_value\"], ascending=[False, True])\n",
    "\n",
    "# Visualizando dados\n",
    "df_expensive_items.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+---------+\n",
      "|          id_produto|vlr_produto|vlr_frete|\n",
      "+--------------------+-----------+---------+\n",
      "|8a3254bee785a526d...|       0.85|    18.23|\n",
      "|8a3254bee785a526d...|       0.85|    18.23|\n",
      "|8a3254bee785a526d...|       0.85|     22.3|\n",
      "|05b515fdc76e888aa...|        1.2|     7.89|\n",
      "|05b515fdc76e888aa...|        1.2|     7.89|\n",
      "+--------------------+-----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Retornando itens mais baratos\n",
    "df_cheap_items = df_order_items.selectExpr(\n",
    "    \"product_id AS id_produto\",\n",
    "    \"price AS vlr_produto\",\n",
    "    \"freight_value AS vlr_frete\"\n",
    ").orderBy(\"vlr_produto\")\n",
    "\n",
    "# Visualizando dados\n",
    "df_cheap_items.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### groupBy() e agg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+----------+---------+---------+---------+-----------------+\n",
      "|product_id                      |qtd_vendas|avg_price|max_price|min_price|dif_max_min_price|\n",
      "+--------------------------------+----------+---------+---------+---------+-----------------+\n",
      "|aca2eb7d00ea1a7b8ebd4e68314663af|527       |71.36    |109.9    |69.9     |40.0             |\n",
      "|99a4788cb24856965c36a24e339b6058|488       |88.17    |89.9     |74.0     |15.9             |\n",
      "|422879e10f46682990de24d770e7f83d|484       |54.91    |59.9     |49.0     |10.9             |\n",
      "|389d119b48cf3043d311335e499d9c6b|392       |54.7     |59.9     |49.0     |10.9             |\n",
      "|368c6c730842d78016ad823897a372db|388       |54.27    |59.9     |49.0     |10.9             |\n",
      "+--------------------------------+----------+---------+---------+---------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import count, avg, max, min, round\n",
    "\n",
    "# Retornando os produtos mais caros vendidos\n",
    "df_items_prep = df_order_items.groupBy(\"product_id\").agg(\n",
    "    count(\"*\").alias(\"qtd_vendas\"),\n",
    "    round(avg(\"price\"), 2).alias(\"avg_price\"),\n",
    "    expr(\"max(price) AS max_price\"),\n",
    "    min(\"price\").alias(\"min_price\"),\n",
    "    expr(\"round(max(price) - min(price), 2) AS dif_max_min_price\")\n",
    ").orderBy(\"qtd_vendas\", ascending=False)\n",
    "\n",
    "# Visualizando dados\n",
    "df_items_prep.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 33:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|payment_type|avg(payment_value)|\n",
      "+------------+------------------+\n",
      "|      boleto|145.03443540234633|\n",
      "| not_defined|               0.0|\n",
      "| credit_card|163.31902063935996|\n",
      "|     voucher| 65.70335411255414|\n",
      "|  debit_card|142.57017004578165|\n",
      "+------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Média de pagamentos por tipo\n",
    "df_payment_types = df_order_payments.groupBy(\"payment_type\").avg(\"payment_value\")\n",
    "\n",
    "# Visualizando dados\n",
    "df_payment_types.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+-----------------+-----------------+\n",
      "|payment_type|qtd_pgtos|avg_payment_value|sum_payment_value|\n",
      "+------------+---------+-----------------+-----------------+\n",
      "| credit_card|    76795|           163.32|    1.254208419E7|\n",
      "|      boleto|    19784|           145.03|       2869361.27|\n",
      "|     voucher|     5775|             65.7|        379436.87|\n",
      "|  debit_card|     1529|           142.57|        217989.79|\n",
      "| not_defined|        3|              0.0|              0.0|\n",
      "+------------+---------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Média de pagamentos por tipo\n",
    "df_payment_types = df_order_payments\\\n",
    "    .groupBy(\"payment_type\").agg(\n",
    "        expr(\"count(1) AS qtd_pgtos\"),\n",
    "        expr(\"round(avg(payment_value), 2) AS avg_payment_value\"),\n",
    "        expr(\"round(sum(payment_value), 2) AS sum_payment_value\")\n",
    "    ).sort(\"qtd_pgtos\", ascending=False)\n",
    "\n",
    "# Visualizando dados\n",
    "df_payment_types.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Juntando Dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### union()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registros de produtos na categoria PERFURMARIA: 868\n",
      "Registros de produtos na categoria ARTES: 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 45:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Registros no DataFrame após união: 923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Gerando DataFrames específicos para posterior união\n",
    "df_perfurmaria = df_products.where(expr(\"product_category_name = 'perfumaria'\"))\n",
    "df_artes = df_products.where(expr(\"product_category_name = 'artes'\"))\n",
    "\n",
    "# Contabilizando registros\n",
    "print(f'Registros de produtos na categoria PERFURMARIA: {df_perfurmaria.count()}')\n",
    "print(f'Registros de produtos na categoria ARTES: {df_artes.count()}')\n",
    "\n",
    "# Unindo DataFrames\n",
    "df_perfurmaria_artes = df_perfurmaria.union(df_artes)\n",
    "print(f'\\nRegistros no DataFrame após união: {df_perfurmaria_artes.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Union can only be performed on tables with the same number of columns, but the first table has 8 columns and the second table has 5 columns;\n'Union false, false\n:- Relation [order_id#17,customer_id#18,order_status#19,order_purchase_timestamp#20,order_approved_at#21,order_delivered_carrier_date#22,order_delivered_customer_date#23,order_estimated_delivery_date#24] csv\n+- Relation [order_id#81,payment_sequential#82,payment_type#83,payment_installments#84,payment_value#85] csv\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art16-visao-gerao-transformacoes.ipynb Cell 42\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art16-visao-gerao-transformacoes.ipynb#ch0000041?line=0'>1</a>\u001b[0m \u001b[39m# Unindo DataFrames diferentes\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/hadoop/dev/panini-tech-lab/tech/spark/notebooks/art16-visao-gerao-transformacoes.ipynb#ch0000041?line=1'>2</a>\u001b[0m df_orders\u001b[39m.\u001b[39;49munion(df_order_payments)\n",
      "File \u001b[0;32m~/dev/python/venvs/pyspark-venv/lib/python3.10/site-packages/pyspark/sql/dataframe.py:2257\u001b[0m, in \u001b[0;36mDataFrame.union\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   2247\u001b[0m \u001b[39m@since\u001b[39m(\u001b[39m2.0\u001b[39m)\n\u001b[1;32m   2248\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39munion\u001b[39m(\u001b[39mself\u001b[39m, other: \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mDataFrame\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   2249\u001b[0m     \u001b[39m\"\"\"Return a new :class:`DataFrame` containing union of rows in this and another\u001b[39;00m\n\u001b[1;32m   2250\u001b[0m \u001b[39m    :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   2251\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2255\u001b[0m \u001b[39m    Also as standard in SQL, this function resolves columns by position (not by name).\u001b[39;00m\n\u001b[1;32m   2256\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2257\u001b[0m     \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49munion(other\u001b[39m.\u001b[39;49m_jdf), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m~/dev/python/venvs/pyspark-venv/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/dev/python/venvs/pyspark-venv/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Union can only be performed on tables with the same number of columns, but the first table has 8 columns and the second table has 5 columns;\n'Union false, false\n:- Relation [order_id#17,customer_id#18,order_status#19,order_purchase_timestamp#20,order_approved_at#21,order_delivered_carrier_date#22,order_delivered_customer_date#23,order_estimated_delivery_date#24] csv\n+- Relation [order_id#81,payment_sequential#82,payment_type#83,payment_installments#84,payment_value#85] csv\n"
     ]
    }
   ],
   "source": [
    "# Unindo DataFrames diferentes\n",
    "df_orders.union(df_order_payments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|payment_type|  qtd|\n",
      "+------------+-----+\n",
      "|      boleto|19784|\n",
      "| not_defined|    3|\n",
      "| credit_card|76795|\n",
      "|     voucher| 5775|\n",
      "|  debit_card| 1529|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Unindo DataFrames diferentes com mesmo numero de colunas\n",
    "df_incorrect_union = df_order_payments.union(df_customers)\n",
    "\n",
    "# Aplicando algumas validações\n",
    "df_order_payments.groupBy(\"payment_type\").agg(expr(\"count(1) AS qtd\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|product_category_name |qty_cat_sales|sum_cat_sales|avg_cat_price|max_cat_price|min_cat_price|\n",
      "+----------------------+-------------+-------------+-------------+-------------+-------------+\n",
      "|cama_mesa_banho       |11115        |1036988.68   |93.3         |1999.98      |6.99         |\n",
      "|beleza_saude          |9670         |1258681.34   |130.16       |3124.0       |1.2          |\n",
      "|esporte_lazer         |8641         |988048.97    |114.34       |4059.0       |4.5          |\n",
      "|moveis_decoracao      |8334         |729762.49    |87.56        |1899.0       |4.9          |\n",
      "|informatica_acessorios|7827         |911954.32    |116.51       |3699.99      |3.9          |\n",
      "|utilidades_domesticas |6964         |632248.66    |90.79        |6735.0       |3.06         |\n",
      "|relogios_presentes    |5991         |1205005.68   |201.14       |3999.9       |8.99         |\n",
      "|telefonia             |4545         |323667.53    |71.21        |2428.0       |5.0          |\n",
      "|ferramentas_jardim    |4347         |485256.46    |111.63       |3930.0       |6.35         |\n",
      "|automotivo            |4235         |592720.11    |139.96       |2258.0       |3.49         |\n",
      "+----------------------+-------------+-------------+-------------+-------------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Analisando média de preços de produtos\n",
    "df_product_category = df_order_items.join(\n",
    "    other=df_products,\n",
    "    on=[df_order_items.product_id == df_products.product_id],\n",
    "    how=\"left\"\n",
    ").groupBy(\"product_category_name\").agg(\n",
    "    expr(\"count(1) AS qty_cat_sales\"),\n",
    "    expr(\"round(sum(price), 2) AS sum_cat_sales\"),\n",
    "    expr(\"round(avg(price), 2) AS avg_cat_price\"),\n",
    "    expr(\"max(price) AS max_cat_price\"),\n",
    "    expr(\"min(price) AS min_cat_price\")\n",
    ").orderBy(\"qty_cat_sales\", ascending=False)\n",
    "\n",
    "# Visualizando dados\n",
    "df_product_category.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------+---------------+---------------+---------+---------+\n",
      "|id_pedido                       |categoria_produto |cidade_vendedor|estado_vendedor|vlr_venda|vlr_frete|\n",
      "+--------------------------------+------------------+---------------+---------------+---------+---------+\n",
      "|00010242fe8c5a6d1ba2dd792cb16214|cool_stuff        |volta redonda  |SP             |58.9     |13.29    |\n",
      "|00018f77f2f0320c557190d7a144bdd3|pet_shop          |sao paulo      |SP             |239.9    |19.93    |\n",
      "|000229ec398224ef6ca0657da4fc703e|moveis_decoracao  |borda da mata  |MG             |199.0    |17.87    |\n",
      "|00024acbcdf0a6daa1e931b038114c75|perfumaria        |franca         |SP             |12.99    |12.79    |\n",
      "|00042b26cf59d7ce69dfabb4e55b4fd9|ferramentas_jardim|loanda         |PR             |199.9    |18.14    |\n",
      "+--------------------------------+------------------+---------------+---------------+---------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cruzando dados de múltiplos conjuntos\n",
    "df_sales = df_order_items.join(\n",
    "    other=df_products,\n",
    "    on=[df_order_items.product_id == df_products.product_id],\n",
    "    how='left'\n",
    ").join(\n",
    "    other=df_sellers,\n",
    "    on=[df_order_items.seller_id == df_sellers.seller_id],\n",
    "    how='left'\n",
    ").selectExpr(\n",
    "    \"order_id AS id_pedido\",\n",
    "    \"product_category_name AS categoria_produto\",\n",
    "    \"seller_city AS cidade_vendedor\",\n",
    "    \"seller_state AS estado_vendedor\",\n",
    "    \"price AS vlr_venda\",\n",
    "    \"freight_value AS vlr_frete\"\n",
    ")\n",
    "\n",
    "# Visualizando resultado\n",
    "df_sales.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 139:==============>                                          (1 + 3) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+------------+-----------+----------+\n",
      "|anomes_pedido|anomesdia_pedido|qtd_pedidos|qtd_produtos|soma_vendas|soma_frete|\n",
      "+-------------+----------------+-----------+------------+-----------+----------+\n",
      "|       201609|        20160915|          1|           3|     134.97|      8.49|\n",
      "|       201610|        20161003|          7|           7|     441.98|    117.55|\n",
      "|       201610|        20161004|         54|          63|    8595.89|   1225.53|\n",
      "|       201610|        20161005|         35|          48|    6169.77|   1039.73|\n",
      "|       201610|        20161006|         41|          47|    5889.96|    908.94|\n",
      "|       201610|        20161007|         38|          44|    6075.35|    749.11|\n",
      "|       201610|        20161008|         36|          41|    7592.89|    883.04|\n",
      "|       201610|        20161009|         20|          26|     2399.7|    504.49|\n",
      "|       201610|        20161010|         34|          37|    3159.57|    737.16|\n",
      "|       201612|        20161223|          1|           1|       10.9|      8.72|\n",
      "|       201701|        20170105|         32|          32|      396.9|    310.37|\n",
      "|       201701|        20170106|          4|           4|     916.38|      71.9|\n",
      "|       201701|        20170107|          4|           5|     1351.9|     96.09|\n",
      "|       201701|        20170108|          4|           4|     449.78|     58.07|\n",
      "|       201701|        20170109|          5|           6|     673.79|    100.72|\n",
      "+-------------+----------------+-----------+------------+-----------+----------+\n",
      "only showing top 15 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Criando tabelas temporárias\n",
    "df_orders.createOrReplaceTempView(\"tbl_orders\")\n",
    "df_order_items.createOrReplaceTempView(\"tbl_order_items\")\n",
    "\n",
    "# Gerando visão história de vendas\n",
    "df_ecommerce_hist = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        anomes_pedido,\n",
    "        anomesdia_pedido,\n",
    "        count(DISTINCT id_pedido) AS qtd_pedidos,\n",
    "        count(id_item) AS qtd_produtos,\n",
    "        round(sum(vlr_item), 2) AS soma_vendas,\n",
    "        round(sum(vlr_frete), 2) AS soma_frete\n",
    "\n",
    "    FROM (\n",
    "        SELECT\n",
    "            o.order_id AS id_pedido,\n",
    "            i.product_id AS id_item,\n",
    "            i.price AS vlr_item,\n",
    "            i.freight_value AS vlr_frete,\n",
    "            date_format(o.order_purchase_timestamp, 'yyyyMM') AS anomes_pedido,\n",
    "            date_format(o.order_purchase_timestamp, 'yyyyMMdd') AS anomesdia_pedido\n",
    "\n",
    "        FROM tbl_orders AS o\n",
    "\n",
    "        LEFT JOIN tbl_order_items AS i\n",
    "            ON o.order_id = i.order_id\n",
    "\n",
    "        WHERE lower(o.order_status) = 'delivered'\n",
    "    )\n",
    "\n",
    "    GROUP BY\n",
    "        anomes_pedido,\n",
    "        anomesdia_pedido\n",
    "    \n",
    "    ORDER BY anomesdia_pedido ASC\n",
    "\n",
    "\"\"\")\n",
    "\n",
    "# Visualizando conjunto\n",
    "df_ecommerce_hist.show(15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyspark-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3a59d43698a1c21b8ea695e9dada7fb9edd9002f05e8c3363eca03517868e40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

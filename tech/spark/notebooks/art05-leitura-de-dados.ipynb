{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/19 20:47:53 WARN Utils: Your hostname, panini-ubuntu resolves to a loopback address: 127.0.1.1; using 10.0.0.106 instead (on interface enp3s0)\n",
      "22/07/19 20:47:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/19 20:47:58 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/07/19 20:48:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.0.106:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>art05-leitura-e-escrita</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f4f287cf580>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Inicializando sessão\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .appName(\"art05-leitura-e-escrita\")\n",
    "    .getOrCreate())\n",
    "\n",
    "# Definindo variáveis de diretório para leitura dos arquivos\n",
    "parent_dir = ''.join(os.path.pardir + \"/\") * 3\n",
    "flights_dir = os.path.join(parent_dir, 'data/flights-data/summary-data')\n",
    "\n",
    "# Definindo variáveis de leitura para cada formato\n",
    "csv_data_path = os.path.join(flights_dir, 'csv/2015-summary.csv')\n",
    "json_data_path = os.path.join(flights_dir, 'json/2015-summary.json')\n",
    "orc_data_path = os.path.join(flights_dir, 'orc/2010-summary.orc/part-r-00000-2c4f7d96-e703-4de3-af1b-1441d172c80f.snappy.orc')\n",
    "avro_data_path = os.path.join(flights_dir, 'avro/part-00000-tid-7128780539805330008-467d814d-6f80-4951-a951-f9f7fb8e3930-1434-1-c000.avro')\n",
    "parquet_data_path = os.path.join(flights_dir, 'parquet/2010-summary.parquet/part-r-00000-1a9822ba-b8fb-4d8e-844a-ea30d0801b9e.gz.parquet')\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo Arquivo CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando a leitura de arquivo CSV com opções características\n",
    "df_csv = (spark.read.format(\"csv\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(csv_data_path))\n",
    "\n",
    "# Visualizando resultado\n",
    "df_csv.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificando schema\n",
    "df_csv.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leitura de arquivo JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando tipos primtiivos\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "# Definindo schema\n",
    "flights_schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), nullable=True),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), nullable=True),\n",
    "    StructField(\"count\", IntegerType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Realizando a leitura de arquivo JSON com schema explícito\n",
    "df_json = (spark.read.format(\"json\")\n",
    "    .schema(flights_schema)\n",
    "    .load(json_data_path))\n",
    "\n",
    "# Verificando schema\n",
    "print(df_json.printSchema())\n",
    "\n",
    "# Visualizando resultado\n",
    "df_json.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lendo arquivos ORC e PARQUET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lendo arquivo ORC\n",
    "df_orc = spark.read.format(\"orc\").load(orc_data_path)\n",
    "\n",
    "# Lendo arquivos parquert\n",
    "df_parquet = spark.read.format(\"parquet\").load(parquet_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validando schema\n",
    "print(f'Schemas iguais? {df_orc.schema == df_parquet.schema}\\n')\n",
    "\n",
    "# Amostra orc\n",
    "df_orc.show(5)\n",
    "\n",
    "# Amostra parquet\n",
    "df_parquet.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bônus: Leitura de S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/19 20:48:20 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  252|\n",
      "|            Egypt|      United States|   13|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|   25|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Configurando sessão para leitura de dados do s3\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"AKIASG54ID4RYWWGUQWG\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"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\")\n",
    "#spark._jsc.hadoopConfiguration().set(\"com.amazonaws.services.s3.enableV4\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"s3.amazonaws.com\")\n",
    "\n",
    "df_s3 = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(\"s3a://aws-training-toolkit-152329264931-us-east-1/data/flights-data/summary-data/csv/2012-summary.csv\")\n",
    "df_s3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  252|\n",
      "|            Egypt|      United States|   13|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|   25|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_s3.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyspark-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3a59d43698a1c21b8ea695e9dada7fb9edd9002f05e8c3363eca03517868e40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "import os\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")\n",
    "\n",
    "# Criando sessão Spark\n",
    "spark = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .appName(\"art11\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Definindo variáveis de diretório\n",
    "home_path = os.path.expanduser('~')\n",
    "data_path = os.path.join(home_path, 'dev/panini-tech-lab/data/flights-data/summary-data/csv/2015-summary.csv')\n",
    "\n",
    "# Definindo schema para o arquivo CSV a ser lido\n",
    "data_schema = StructType([\n",
    "    StructField(\"DEST_COUNTRY_NAME\", StringType(), nullable=True, metadata={\"description\": \"País de destino dos vôos contabilizados\"}),\n",
    "    StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), nullable=True, metadata={\"description\": \"País de origem dos vôos contabilizados\"}),\n",
    "    StructField(\"count\", IntegerType(), nullable=True, metadata={\"description\": \"Contagem total de vôos entre os países de origem e de destino do registro\"})\n",
    "])\n",
    "\n",
    "# Realizando a leitura dos dados\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .schema(data_schema)\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(data_path)\n",
    ")\n",
    "\n",
    "# Criando tabela temporária\n",
    "df.createOrReplaceTempView(\"tbl_flights\")\n",
    "\n",
    "# Verificando amostra dos dados\n",
    "df.printSchema()\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      " |-- lit_one: integer (nullable = false)\n",
      " |-- lit_two: integer (nullable = false)\n",
      " |-- lit_str: string (nullable = false)\n",
      " |-- lit_str_expr: string (nullable = false)\n",
      "\n",
      "+-----------------+-------------------+-----+-------+-------+-------+------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|lit_one|lit_two|lit_str|lit_str_expr|\n",
      "+-----------------+-------------------+-----+-------+-------+-------+------------+\n",
      "|    United States|            Romania|   15|      1|      2|    str|    str_expr|\n",
      "|    United States|            Croatia|    1|      1|      2|    str|    str_expr|\n",
      "|    United States|            Ireland|  344|      1|      2|    str|    str_expr|\n",
      "|            Egypt|      United States|   15|      1|      2|    str|    str_expr|\n",
      "|    United States|              India|   62|      1|      2|    str|    str_expr|\n",
      "+-----------------+-------------------+-----+-------+-------+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import lit, col, expr\n",
    "\n",
    "# Criando literais em consultas\n",
    "df_lit = df.select(\n",
    "    col(\"*\"),\n",
    "    lit(1).alias(\"lit_one\"),\n",
    "    expr(\"2 as lit_two\"),\n",
    "    lit(\"str\").alias(\"lit_str\"),\n",
    "    expr(\"'str_expr' AS lit_str_expr\")\n",
    ")\n",
    "\n",
    "# Verificando schema\n",
    "df_lit.printSchema()\n",
    "df_lit.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- lit_int: integer (nullable = false)\n",
      " |-- lit_str_cast: boolean (nullable = false)\n",
      "\n",
      "+-------+------------+\n",
      "|lit_int|lit_str_cast|\n",
      "+-------+------------+\n",
      "|      1|        true|\n",
      "|      1|        true|\n",
      "|      1|        true|\n",
      "|      1|        true|\n",
      "|      1|        true|\n",
      "+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando literais e alterando tipo primitivo\n",
    "df_casting1 = df.select(\n",
    "    lit(1).alias(\"lit_int\"),\n",
    "    lit(1).cast(\"string\").alias(\"lit_str_cast\")\n",
    ")\n",
    "\n",
    "# Visualizando schema e amostra\n",
    "df_casting1.printSchema()\n",
    "df_casting1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(lit_int=1, lit_str_cast='1')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coletando uma linha\n",
    "df_casting1.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- int: integer (nullable = false)\n",
      " |-- str: string (nullable = false)\n",
      " |-- double: double (nullable = false)\n",
      " |-- bool_true: boolean (nullable = false)\n",
      " |-- bool_false: boolean (nullable = false)\n",
      "\n",
      "+---+---+------+---------+----------+\n",
      "|int|str|double|bool_true|bool_false|\n",
      "+---+---+------+---------+----------+\n",
      "|  1|  1|   1.0|     true|     false|\n",
      "|  1|  1|   1.0|     true|     false|\n",
      "|  1|  1|   1.0|     true|     false|\n",
      "|  1|  1|   1.0|     true|     false|\n",
      "|  1|  1|   1.0|     true|     false|\n",
      "+---+---+------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[Row(int=1, str='1', double=1.0, bool_true=True, bool_false=False)]\n"
     ]
    }
   ],
   "source": [
    "# Importando tipos primitivos\n",
    "from tokenize import Double\n",
    "from pyspark.sql.types import StringType, DoubleType, BooleanType\n",
    "\n",
    "# Criando literais e alterando tipo primitivo\n",
    "df_casting2 = df.select(\n",
    "    lit(1).alias(\"int\"),\n",
    "    lit(1).cast(StringType()).alias(\"str\"),\n",
    "    lit(1).cast(DoubleType()).alias(\"double\"),\n",
    "    lit(1).cast(BooleanType()).alias(\"bool_true\"),\n",
    "    lit(0).cast(BooleanType()).alias(\"bool_false\")\n",
    ")\n",
    "\n",
    "# Visualizando schema e amostra\n",
    "df_casting2.printSchema()\n",
    "df_casting2.show(5)\n",
    "\n",
    "# Coletando registro no driver\n",
    "row = df_casting2.take(1)\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- dest_country_double: double (nullable = true)\n",
      "\n",
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|dest_country_double|\n",
      "+-----------------+-------------------+\n",
      "|    United States|               null|\n",
      "|    United States|               null|\n",
      "|    United States|               null|\n",
      "|            Egypt|               null|\n",
      "|    United States|               null|\n",
      "+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[Row(DEST_COUNTRY_NAME='United States', dest_country_double=None)]\n"
     ]
    }
   ],
   "source": [
    "# Gerando erros de conversão\n",
    "df_casting3 = df.select(\n",
    "    \"DEST_COUNTRY_NAME\",\n",
    "    col(\"DEST_COUNTRY_NAME\").cast(DoubleType()).alias(\"dest_country_double\")\n",
    ")\n",
    "\n",
    "# Verificando tipo primitivo e amostra\n",
    "df_casting3.printSchema()\n",
    "df_casting3.show(5)\n",
    "\n",
    "# Coletando registro no driver\n",
    "row = df_casting3.take(1)\n",
    "print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- integer: integer (nullable = false)\n",
      " |-- string: string (nullable = false)\n",
      " |-- double: double (nullable = false)\n",
      " |-- bool_true: boolean (nullable = false)\n",
      " |-- bool_false: boolean (nullable = false)\n",
      "\n",
      "+-------+------+------+---------+----------+\n",
      "|integer|string|double|bool_true|bool_false|\n",
      "+-------+------+------+---------+----------+\n",
      "|      1|     1|   1.0|     true|      true|\n",
      "|      1|     1|   1.0|     true|      true|\n",
      "|      1|     1|   1.0|     true|      true|\n",
      "|      1|     1|   1.0|     true|      true|\n",
      "|      1|     1|   1.0|     true|      true|\n",
      "+-------+------+------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "[Row(integer=1, string='1', double=1.0, bool_true=True, bool_false=True)]\n"
     ]
    }
   ],
   "source": [
    "# Convertendo via SparkSQL\n",
    "df_casting4 = spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        1 AS integer,\n",
    "        cast(1 AS string) AS string,\n",
    "        cast(1 AS double) AS double,\n",
    "        cast(1 AS boolean) AS bool_true,\n",
    "        cast(1 AS boolean) AS bool_false\n",
    "\n",
    "    FROM tbl_flights\n",
    "\"\"\")\n",
    "\n",
    "# Verificando schema e amostra\n",
    "df_casting4.printSchema()\n",
    "df_casting4.show(5)\n",
    "\n",
    "# Coletando amostra para o driver\n",
    "print(df_casting4.take(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+----+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|flag|flag2|\n",
      "+-----------------+-------------------+-----+----+-----+\n",
      "|    United States|            Romania|   15|   1|    0|\n",
      "|    United States|            Croatia|    1|   1|    0|\n",
      "|    United States|            Ireland|  344|   1|    1|\n",
      "|            Egypt|      United States|   15|   1|    0|\n",
      "|    United States|              India|   62|   1|    1|\n",
      "+-----------------+-------------------+-----+----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import lit, col, expr, when\n",
    "\n",
    "# Criando literais\n",
    "df.select(\n",
    "    \"*\",\n",
    "    lit(1).alias(\"flag\"),\n",
    "    when((col(\"count\") > 30), lit(1)).otherwise(lit(0)).alias(\"flag2\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pyspark-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3a59d43698a1c21b8ea695e9dada7fb9edd9002f05e8c3363eca03517868e40"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f23ea4d",
   "metadata": {},
   "source": [
    "**_Objetivo:_** Neste notebook, serão consolidados códigos para explorações práticas envolvendo o contéudo presente no capítulo 6 do livro Spark - The Definitive Guide: Working with Different Types of Data. No cenário proposto, exemplos de operações com diferentes tipos primitivos são mostrados, desde booleanos, até tipos complexos. Além disso, também serão mostrados exemplos de utilização de funções definidas pelo usuário (UDFs) para transformações adicionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d69692c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-CJKTBO0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1aaa013cc70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Definindo variáveis de diretório\n",
    "DATA_PATH = '../book-github-resources/Spark-The-Definitive-Guide-master/data/retail-data/by-day/2010-12-01.csv'\n",
    "\n",
    "# Inicializando sessão\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "276e4c3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|   17850.0|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|   17850.0|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: double (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lendo base de dados \n",
    "df = spark.read.format('csv')\\\n",
    "    .option('header', 'true')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .load(DATA_PATH)\n",
    "\n",
    "# Criando view\n",
    "df.createOrReplaceTempView('vw_retail_data')\n",
    "\n",
    "# Verificando dados e schema\n",
    "df.show(2)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bd9072c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import col, expr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d411c12a",
   "metadata": {},
   "source": [
    "# Trabalhando com Booleanos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b68c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+\n",
      "|InvoiceNo|Description              |\n",
      "+---------+-------------------------+\n",
      "|536366   |HAND WARMER UNION JACK   |\n",
      "|536366   |HAND WARMER RED POLKA DOT|\n",
      "+---------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrando dados com referência de colunas\n",
    "df.where(col(\"InvoiceNo\") != 536365)\\\n",
    "    .select(\"InvoiceNo\", \"Description\")\\\n",
    "    .show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45a55498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------------------+\n",
      "|InvoiceNo|Description              |\n",
      "+---------+-------------------------+\n",
      "|536366   |HAND WARMER UNION JACK   |\n",
      "|536366   |HAND WARMER RED POLKA DOT|\n",
      "+---------+-------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrando dados direto com expressões\n",
    "df.where(\"InvoiceNo <> 536365\")\\\n",
    "    .select(\"InvoiceNo\", \"Description\")\\\n",
    "    .show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73756409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(NOT (InvoiceNo = 536365))'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr(\"InvoiceNo <> 536365\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9736b76e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(NOT (InvoiceNo = 536365))'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"InvoiceNo\") != 536365"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0719cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+---------+\n",
      "|InvoiceNo|StockCode|   Description|UnitPrice|\n",
      "+---------+---------+--------------+---------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|   569.77|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|   607.49|\n",
      "+---------+---------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função instr\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "# Criação de filtros\n",
    "stockFilter = col(\"StockCode\").contains(\"DOT\")\n",
    "priceFilter = expr(\"UnitPrice\") > 600\n",
    "descFilter = instr(\"Description\", \"POSTAGE\") >= 1\n",
    "\n",
    "# Lista de seleção de colunas\n",
    "selectList = [\"InvoiceNo\", \"StockCode\", \"Description\", \"UnitPrice\"]\n",
    "\n",
    "# Aplicando filtros\n",
    "df.where(stockFilter)\\\n",
    "    .where(priceFilter | descFilter)\\\n",
    "    .select(selectList)\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "76059ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+---------+\n",
      "|InvoiceNo|StockCode|   Description|UnitPrice|\n",
      "+---------+---------+--------------+---------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|   569.77|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|   607.49|\n",
      "+---------+---------+--------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Análogo em SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT InvoiceNo, StockCode, Description, UnitPrice\n",
    "    FROM vw_retail_data\n",
    "    WHERE StockCode IN (\"DOT\")\n",
    "        AND (UnitPrice > 600 OR instr(Description, \"POSTAGE\") >= 1)\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ddf769c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------+---------+-----------+\n",
      "|InvoiceNo|StockCode|   Description|UnitPrice|isExpensive|\n",
      "+---------+---------+--------------+---------+-----------+\n",
      "|   536544|      DOT|DOTCOM POSTAGE|   569.77|       true|\n",
      "|   536592|      DOT|DOTCOM POSTAGE|   607.49|       true|\n",
      "+---------+---------+--------------+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filtrando dados a partir de coluna booleana\n",
    "df.withColumn('isExpensive', stockFilter & (priceFilter | descFilter))\\\n",
    "    .where('isExpensive')\\\n",
    "    .select(selectList + ['isExpensive'])\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfce552",
   "metadata": {},
   "source": [
    "# Trabalhando com Numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ea6602e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------+\n",
      "|Quantity|UnitPrice|realQuantity|\n",
      "+--------+---------+------------+\n",
      "|       6|     2.55|      239.09|\n",
      "|       6|     3.39|      418.72|\n",
      "|       8|     2.75|       489.0|\n",
      "+--------+---------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import expr, pow, round\n",
    "\n",
    "# Criando novo campo com base em fórmula (jeito 1)\n",
    "fabricatedQuantity = round(pow(col(\"Quantity\") * col(\"UnitPrice\"), 2) + 5, 2)\n",
    "df.select(\n",
    "    \"Quantity\", \"UnitPrice\", \n",
    "    fabricatedQuantity.alias(\"realQuantity\")\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87dfcabb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+------------+\n",
      "|Quantity|UnitPrice|realQuantity|\n",
      "+--------+---------+------------+\n",
      "|       6|     2.55|      239.09|\n",
      "|       6|     3.39|      418.72|\n",
      "|       8|     2.75|       489.0|\n",
      "+--------+---------+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando novo campo com base em fórmula (jeito 2)\n",
    "df.selectExpr(\n",
    "    \"Quantity\", \"UnitPrice\", \n",
    "    \"round(power((Quantity * UnitPrice), 2) + 5, 2) AS realQuantity\"\n",
    ").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38bd1131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|        PriceQtyCorr|\n",
      "+--------------------+\n",
      "|-0.04112314436835551|\n",
      "+--------------------+\n",
      "\n",
      "+--------------------+\n",
      "|        PriceQtyCorr|\n",
      "+--------------------+\n",
      "|-0.04112314436835551|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import corr\n",
    "\n",
    "# Correlação entre variáveis via select\n",
    "df.select(corr(\"Quantity\", \"UnitPrice\").alias(\"PriceQtyCorr\")).show()\n",
    "\n",
    "# Correlação entre variáveis via selectExpr\n",
    "df.selectExpr(\"corr(Quantity, UnitPrice) AS PriceQtyCorr\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "599b861f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+\n",
      "|summary|          Quantity|         UnitPrice|        CustomerID|\n",
      "+-------+------------------+------------------+------------------+\n",
      "|  count|              3108|              3108|              1968|\n",
      "|   mean| 8.627413127413128| 4.151946589446603|15661.388719512195|\n",
      "| stddev|26.371821677029203|15.638659854603892|1854.4496996893627|\n",
      "|    min|               -24|               0.0|           12431.0|\n",
      "|    max|               600|            607.49|           18229.0|\n",
      "+-------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Coletando colunas numéricas do DataFrame\n",
    "num_cols = [s.name for s in df.schema if s.dataType.typeName() != 'string']\n",
    "\n",
    "# Computando estatísticas de colunas numéricas\n",
    "df.select(num_cols).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a4109af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+----------------+-----------------+-----------------+------------------+\n",
      "|Quantity_count|UnitPrice_count|CustomerID_count|    Quantity_mean|   UnitPrice_mean|   CustomerID_mean|\n",
      "+--------------+---------------+----------------+-----------------+-----------------+------------------+\n",
      "|          3108|           3108|            1968|8.627413127413128|4.151946589446603|15661.388719512195|\n",
      "+--------------+---------------+----------------+-----------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções estatísticas\n",
    "from pyspark.sql.functions import count, mean, stddev_pop, min, max\n",
    "\n",
    "# Calculando estatísticas na unha\n",
    "function_list = [count, mean, stddev_pop, min, max]\n",
    "stats_list = [func(col).alias(col + '_' + func.__name__) for func in function_list for col in num_cols]\n",
    "\n",
    "# Realizando consulta (mostrando apenas as colunas de count e mean - espaço)\n",
    "df.select(stats_list[:6]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed8d2231",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(StockCode_freqItems=['90214E', '20728', '20755', '21703', '22113', '22524', '22041', '72803A', '72798C', '90181B', '21756', '22694', '90206C', '20970', '21624', '90209C', '84744', '82494L', '22952', '20682', '22583', '21705', '20679', '22220', '90177E', '90214A', '22448', '90214S', '22121', '22802', '84970L', '72818', '90192', '90200C', '22910', '21380', '90211A', '21137', '35271S', '84926A', '20765', '22384', '21524', '22165', '22366', '21221', '21704', '22519', '85035C', '21967', '22114', '22909', '22900', '22447', '21577', '21877', '20726', '85034A', 'DOT', '84658', '21472', '22804', '22222', '72802C', '21739', '22467', '90214H', '22785', '22446', '22197', '20665', '21733', '22731', '21709', '22086', '40001', '85123A'])]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Aplicando crosstab\n",
    "df.stat.freqItems([\"StockCode\"]).take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32d4d4b",
   "metadata": {},
   "source": [
    "# Trabalhando com Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "484d8ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+\n",
      "|Description                       |TitledDescription                 |\n",
      "+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|White Hanging Heart T-light Holder|\n",
      "|WHITE METAL LANTERN               |White Metal Lantern               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |Cream Cupid Hearts Coat Hanger    |\n",
      "+----------------------------------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import initcap\n",
    "\n",
    "# Aplicando title case\n",
    "df.select(\n",
    "    \"Description\", \n",
    "    initcap(\"Description\").alias(\"TitledDescription\")\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "253ad0e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|Description                       |LowerDescription                  |UpperDescription                  |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|white hanging heart t-light holder|WHITE HANGING HEART T-LIGHT HOLDER|\n",
      "|WHITE METAL LANTERN               |white metal lantern               |WHITE METAL LANTERN               |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |cream cupid hearts coat hanger    |CREAM CUPID HEARTS COAT HANGER    |\n",
      "+----------------------------------+----------------------------------+----------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import lower, upper\n",
    "\n",
    "# Alterando case \n",
    "df.select(\n",
    "    \"Description\",\n",
    "    lower(\"Description\").alias(\"LowerDescription\"),\n",
    "    upper(lower(\"Description\")).alias(\"UpperDescription\")\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9e8dca5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|            original|              titled|               lower|               upper|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|WHITE HANGING HEA...|White Hanging Hea...|white hanging hea...|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN| White Metal Lantern| white metal lantern| WHITE METAL LANTERN|\n",
      "|CREAM CUPID HEART...|Cream Cupid Heart...|cream cupid heart...|CREAM CUPID HEART...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizando via SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        Description AS original,\n",
    "        initcap(Description) AS titled,\n",
    "        lower(Description) AS lower,\n",
    "        upper(lower(Description)) AS upper\n",
    "    \n",
    "    FROM vw_retail_data\n",
    "\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae0f6a00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+----+----------+\n",
      "|    ltrim|    rtrim| trim|lpad|      rpad|\n",
      "+---------+---------+-----+----+----------+\n",
      "|HELLO    |    HELLO|HELLO|  HE|HELLO     |\n",
      "|HELLO    |    HELLO|HELLO|  HE|HELLO     |\n",
      "+---------+---------+-----+----+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import lit, ltrim, rtrim, trim, lpad, rpad\n",
    "\n",
    "# Exemplificando tratativas em string\n",
    "df.select(\n",
    "    ltrim(lit(\"    HELLO    \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"    HELLO    \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"    HELLO    \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 2, \" \").alias(\"lpad\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rpad\")\n",
    ").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c6ddf8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------+-----------+\n",
      "|Description                       |color_clean                       |first_color|\n",
      "+----------------------------------+----------------------------------+-----------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|COLOR HANGING HEART T-LIGHT HOLDER|WHITE      |\n",
      "|WHITE METAL LANTERN               |COLOR METAL LANTERN               |WHITE      |\n",
      "|CREAM CUPID HEARTS COAT HANGER    |CREAM CUPID HEARTS COAT HANGER    |           |\n",
      "+----------------------------------+----------------------------------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import regexp_extract, regexp_replace\n",
    "\n",
    "# Definindo RegEx\n",
    "pattern = \"(BLACK|WHITE|RED|GREEN|BLUE)\"\n",
    "\n",
    "# Construindo consulta\n",
    "df.select(\n",
    "    \"Description\",\n",
    "    regexp_replace(col(\"Description\"), pattern, \"COLOR\").alias(\"color_clean\"),\n",
    "    regexp_extract(col(\"Description\"), pattern, 1).alias(\"first_color\")\n",
    ").show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "56754c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|Description                      |\n",
      "+---------------------------------+\n",
      "|JUMBO  BAG BAROQUE BLACK WHITE   |\n",
      "|WOOD BLACK BOARD ANT WHITE FINISH|\n",
      "|JUMBO  BAG BAROQUE BLACK WHITE   |\n",
      "+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import instr\n",
    "\n",
    "# Criando filtros\n",
    "black_color_filter = col(\"Description\").contains(\"BLACK\")\n",
    "white_color_filter = instr(col(\"Description\"), \"WHITE\") >= 1\n",
    "\n",
    "# Aplicando consulta\n",
    "df.select(\"Description\")\\\n",
    "    .where(black_color_filter & white_color_filter)\\\n",
    "    .show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43f4cd88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+--------+--------+------+--------+-------+\n",
      "|Description                        |is_black|is_white|is_red|is_green|is_blue|\n",
      "+-----------------------------------+--------+--------+------+--------+-------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |false   |true    |false |false   |false  |\n",
      "|WHITE METAL LANTERN                |false   |true    |false |false   |false  |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |false   |false   |false |false   |false  |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|false   |false   |false |false   |false  |\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |false   |true    |true  |false   |false  |\n",
      "+-----------------------------------+--------+--------+------+--------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import locate\n",
    "\n",
    "# Definindo lista de cores\n",
    "find_list = [\"black\", \"white\", \"red\", \"green\", \"blue\"]\n",
    "func_list = [locate(f.upper(), col(\"Description\")).cast(\"boolean\").alias(f\"is_{f}\") for f in find_list]\n",
    "\n",
    "# Aplicando select\n",
    "df.select(\"Description\", *func_list).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc8a9b6",
   "metadata": {},
   "source": [
    "# Trabalhando com Dates e Timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a1043a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+-----------------------+\n",
      "|id |today     |now                    |\n",
      "+---+----------+-----------------------+\n",
      "|0  |2022-02-24|2022-02-24 18:37:20.516|\n",
      "+---+----------+-----------------------+\n",
      "\n",
      "root\n",
      " |-- id: long (nullable = false)\n",
      " |-- today: date (nullable = false)\n",
      " |-- now: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "\n",
    "# Gerando datas e horas\n",
    "df_date = spark.range(1)\\\n",
    "    .withColumn(\"today\", current_date())\\\n",
    "    .withColumn(\"now\", current_timestamp())\n",
    "df_date.show(truncate=False)\n",
    "\n",
    "# Criando view\n",
    "df_date.createOrReplaceTempView(\"tbl_date\")\n",
    "\n",
    "# Verificando schema\n",
    "df_date.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "187f70ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-------------+\n",
      "|     today|today_plus_5|today_minus_5|\n",
      "+----------+------------+-------------+\n",
      "|2022-02-24|  2022-03-01|   2022-02-19|\n",
      "+----------+------------+-------------+\n",
      "\n",
      "+----------+------------+-------------+\n",
      "|     today|today_plus_5|today_minus_5|\n",
      "+----------+------------+-------------+\n",
      "|2022-02-24|  2022-03-01|   2022-02-19|\n",
      "+----------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "\n",
    "# Adicionando datas\n",
    "df_date.select(\n",
    "    \"today\",\n",
    "    date_add(\"today\", 5).alias(\"today_plus_5\"),\n",
    "    date_sub(\"today\", 5).alias(\"today_minus_5\")\n",
    ").show()\n",
    "\n",
    "# Em SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        today,\n",
    "        date_add(today, 5) AS today_plus_5,\n",
    "        date_sub(today, 5) AS today_minus_5\n",
    "    FROM tbl_date\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0249fbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+\n",
      "|     today|  week_ago|datediff|\n",
      "+----------+----------+--------+\n",
      "|2022-02-24|2022-02-17|       7|\n",
      "+----------+----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import datediff, months_between\n",
    "\n",
    "# Aplicando consulta\n",
    "df_date.withColumn(\"week_ago\", date_sub(\"today\", 7))\\\n",
    "    .select(\n",
    "        \"today\",\n",
    "        \"week_ago\",\n",
    "        datediff(\"today\", \"week_ago\").alias(\"datediff\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8ace37d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|months_between|\n",
      "+--------------+\n",
      "|    -9.4516129|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import to_date\n",
    "\n",
    "# Calculando diferença em meses entre duas datas\n",
    "df_date.select(\n",
    "    to_date(lit(\"2022-02-22\")).alias(\"start\"),\n",
    "    to_date(lit(\"2022-12-05\")).alias(\"end\")\n",
    ").select(\n",
    "    months_between(\"start\", \"end\").alias(\"months_between\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e0ddc4fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+--------------+-------------------+\n",
      "| id|date_default|date_formatted|timestamp_formatted|\n",
      "+---+------------+--------------+-------------------+\n",
      "|  0|        null|    2022-02-02|2022-02-02 00:00:00|\n",
      "+---+------------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import to_date, to_timestamp\n",
    "\n",
    "# Conversão de datas\n",
    "spark.range(1)\\\n",
    "    .withColumn(\"date_default\", to_date(lit(\"02-02-2022\")))\\\n",
    "    .withColumn(\"date_formatted\", to_date(lit(\"02-02-2022\"), \"dd-MM-yyyy\"))\\\n",
    "    .withColumn(\"timestamp_formatted\", to_timestamp(lit(\"02-02-2022\"), \"dd-MM-yyyy\"))\\\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "396f8f4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------------------+\n",
      "|     today|  tomorrow|(today > 2022-02-23)|\n",
      "+----------+----------+--------------------+\n",
      "|2022-02-24|2022-02-23|                true|\n",
      "+----------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comparando datas\n",
    "df_date.select(\n",
    "    \"today\",\n",
    "    lit(\"2022-02-23\").alias(\"tomorrow\"),\n",
    "    col(\"today\") > lit(\"2022-02-23\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "559cdfb5",
   "metadata": {},
   "source": [
    "# Trabalhando com Dados Nulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9d8f9e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+----------------------+\n",
      "|col_1|col_2|coalesce(col_1, col_2)|\n",
      "+-----+-----+----------------------+\n",
      "|    1| null|                     1|\n",
      "| null|    2|                     2|\n",
      "+-----+-----+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import coalesce\n",
    "\n",
    "# Gerando DataFrame\n",
    "df_null = spark.createDataFrame(\n",
    "    [(1, None), (None, 2)], [\"col_1\", \"col_2\"]\n",
    ")\n",
    "\n",
    "# Aplicando função\n",
    "df_null.select(\n",
    "    \"col_1\", \"col_2\", \n",
    "    coalesce(\"col_1\", \"col_2\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f35d1766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+------+-------+-------+\n",
      "|col_1| ifnull|nullif|    nvl|   nvl2|\n",
      "+-----+-------+------+-------+-------+\n",
      "|    1|      1|  null|      1|Retorno|\n",
      "| null|Retorno|  null|Retorno|   Else|\n",
      "+-----+-------+------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando view\n",
    "df_null.createOrReplaceTempView(\"tbl_null\")\n",
    "\n",
    "# Testando diversas outras funções\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        col_1,\n",
    "        ifnull(col_1, \"Retorno\") AS ifnull,\n",
    "        nullif(\"teste\", \"teste\") AS nullif,\n",
    "        nvl(col_1, \"Retorno\") AS nvl,\n",
    "        nvl2(col_1, \"Retorno\", \"Else\") AS nvl2\n",
    "    FROM tbl_null\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3d89a32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+-----+\n",
      "|col_1|col_2|col_3|\n",
      "+-----+-----+-----+\n",
      "+-----+-----+-----+\n",
      "\n",
      "+-----+-----+-----+\n",
      "|col_1|col_2|col_3|\n",
      "+-----+-----+-----+\n",
      "|    1| null| null|\n",
      "| null|    2| null|\n",
      "+-----+-----+-----+\n",
      "\n",
      "+-----+-----+-----+\n",
      "|col_1|col_2|col_3|\n",
      "+-----+-----+-----+\n",
      "| null|    2| null|\n",
      "+-----+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Adicionando coluna totalmente nula\n",
    "df_null = df_null.withColumn(\"col_3\", lit(None))\n",
    "\n",
    "# Utilizando drop\n",
    "df_null.na.drop().show()\n",
    "df_null.na.drop(\"all\").show()\n",
    "df_null.na.drop(\"all\", subset=[\"col_2\", \"col_3\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6de0807",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|col_1|col_2|\n",
      "+-----+-----+\n",
      "|    1|    0|\n",
      "|    0|    2|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|col_1|col_2|\n",
      "+-----+-----+\n",
      "|    1| null|\n",
      "|    0|    2|\n",
      "+-----+-----+\n",
      "\n",
      "+-----+-----+\n",
      "|col_1|col_2|\n",
      "+-----+-----+\n",
      "|    1|   10|\n",
      "|   -1|    2|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Preenchendo dados nulos\n",
    "df_null = df_null.select(\"col_1\", \"col_2\")\n",
    "df_null.na.fill(0).show()\n",
    "\n",
    "# Preenchendo dados nulos com subset\n",
    "df_null.na.fill(0, subset=[\"col_1\"]).show()\n",
    "\n",
    "# Preenchendo dados nulos com dicionário\n",
    "fill_dict = {\"col_1\": -1, \"col_2\": 10}\n",
    "df_null.na.fill(fill_dict).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a4dc73",
   "metadata": {},
   "source": [
    "# Tipos Primitivos Complexos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd834ac",
   "metadata": {},
   "source": [
    "## Struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dae900a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+---------+---------------------------------------------+\n",
      "|Description                        |InvoiceNo|complex                                      |\n",
      "+-----------------------------------+---------+---------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365   |{WHITE HANGING HEART T-LIGHT HOLDER, 536365} |\n",
      "|WHITE METAL LANTERN                |536365   |{WHITE METAL LANTERN, 536365}                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365   |{CREAM CUPID HEARTS COAT HANGER, 536365}     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |{KNITTED UNION FLAG HOT WATER BOTTLE, 536365}|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |536365   |{RED WOOLLY HOTTIE WHITE HEART., 536365}     |\n",
      "+-----------------------------------+---------+---------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "# Criando DataFrame com tipo complexo\n",
    "df_complex = df.select(\n",
    "    \"Description\", \n",
    "    \"InvoiceNo\", \n",
    "    struct(\"Description\", \"InvoiceNo\").alias(\"complex\")\n",
    ")\n",
    "df_complex.show(5, truncate=False)\n",
    "\n",
    "# Criando view\n",
    "df_complex.createOrReplaceTempView(\"df_complex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d27cd749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         Description|\n",
      "+--------------------+\n",
      "|WHITE HANGING HEA...|\n",
      "| WHITE METAL LANTERN|\n",
      "|CREAM CUPID HEART...|\n",
      "|KNITTED UNION FLA...|\n",
      "|RED WOOLLY HOTTIE...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+--------------------+---------+\n",
      "|         Description|InvoiceNo|\n",
      "+--------------------+---------+\n",
      "|WHITE HANGING HEA...|   536365|\n",
      "| WHITE METAL LANTERN|   536365|\n",
      "|CREAM CUPID HEART...|   536365|\n",
      "|KNITTED UNION FLA...|   536365|\n",
      "|RED WOOLLY HOTTIE...|   536365|\n",
      "+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+-----------------+\n",
      "|complex.InvoiceNo|\n",
      "+-----------------+\n",
      "|           536365|\n",
      "|           536365|\n",
      "|           536365|\n",
      "|           536365|\n",
      "|           536365|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando atributos de campo struct\n",
    "df_complex.select(\"complex.Description\").show(5)\n",
    "\n",
    "# Selecionando todos os atributos de um struct\n",
    "df_complex.select(\"complex.*\").show(5)\n",
    "\n",
    "# Selecionando atributo de um struct via getField\n",
    "df_complex.select(col(\"complex\").getField(\"InvoiceNo\")).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59caccee",
   "metadata": {},
   "source": [
    "## Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab299074",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+------------------------------------------+\n",
      "|Description                        |split(Description,  , -1)                 |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |\n",
      "|WHITE METAL LANTERN                |[WHITE, METAL, LANTERN]                   |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |[CREAM, CUPID, HEARTS, COAT, HANGER]      |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |[RED, WOOLLY, HOTTIE, WHITE, HEART.]      |\n",
      "+-----------------------------------+------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import split\n",
    "\n",
    "# Splitando texto\n",
    "df.select(\"Description\", split(\"Description\", \" \")).show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "05968a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|first_word|\n",
      "+----------+\n",
      "|     WHITE|\n",
      "|     WHITE|\n",
      "|     CREAM|\n",
      "|   KNITTED|\n",
      "|       RED|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando DataFrame para uso com arrays\n",
    "df_array = df.select(split(\"Description\", \" \").alias(\"split_desc\"))\n",
    "\n",
    "# Selecionando elementos\n",
    "df_array.selectExpr(\"split_desc[0] AS first_word\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b2147413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------------+\n",
      "|split_desc                                |size(split_desc)|\n",
      "+------------------------------------------+----------------+\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |5               |\n",
      "|[WHITE, METAL, LANTERN]                   |3               |\n",
      "|[CREAM, CUPID, HEARTS, COAT, HANGER]      |5               |\n",
      "|[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|6               |\n",
      "|[RED, WOOLLY, HOTTIE, WHITE, HEART.]      |5               |\n",
      "+------------------------------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import size\n",
    "\n",
    "# Elementos em um array\n",
    "df_array.select(\"split_desc\", size(\"split_desc\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7675167",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+---------------------------------+\n",
      "|split_desc                                |array_contains(split_desc, WHITE)|\n",
      "+------------------------------------------+---------------------------------+\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]  |true                             |\n",
      "|[WHITE, METAL, LANTERN]                   |true                             |\n",
      "|[CREAM, CUPID, HEARTS, COAT, HANGER]      |false                            |\n",
      "|[KNITTED, UNION, FLAG, HOT, WATER, BOTTLE]|false                            |\n",
      "|[RED, WOOLLY, HOTTIE, WHITE, HEART.]      |true                             |\n",
      "+------------------------------------------+---------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import array_contains\n",
    "\n",
    "# Verificando se o array contém determinado valor\n",
    "df_array.select(\"split_desc\", array_contains(\"split_desc\", \"WHITE\")).show(5, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e5a20cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+--------+\n",
      "|split_desc                              |exploded|\n",
      "+----------------------------------------+--------+\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|WHITE   |\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|HANGING |\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|HEART   |\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|T-LIGHT |\n",
      "|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|HOLDER  |\n",
      "|[WHITE, METAL, LANTERN]                 |WHITE   |\n",
      "|[WHITE, METAL, LANTERN]                 |METAL   |\n",
      "|[WHITE, METAL, LANTERN]                 |LANTERN |\n",
      "|[CREAM, CUPID, HEARTS, COAT, HANGER]    |CREAM   |\n",
      "|[CREAM, CUPID, HEARTS, COAT, HANGER]    |CUPID   |\n",
      "|[CREAM, CUPID, HEARTS, COAT, HANGER]    |HEARTS  |\n",
      "|[CREAM, CUPID, HEARTS, COAT, HANGER]    |COAT    |\n",
      "|[CREAM, CUPID, HEARTS, COAT, HANGER]    |HANGER  |\n",
      "+----------------------------------------+--------+\n",
      "only showing top 13 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "# \"Explodindo\" array em múltiplas linhas\n",
    "df_array.select(\n",
    "    \"split_desc\",\n",
    "    explode(\"split_desc\").alias(\"exploded\")\n",
    ").show(13, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "092545c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+----------------------------------------+--------+\n",
      "|Description                       |splitted                                |exploded|\n",
      "+----------------------------------+----------------------------------------+--------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|WHITE   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|HANGING |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|HEART   |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|T-LIGHT |\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|[WHITE, HANGING, HEART, T-LIGHT, HOLDER]|HOLDER  |\n",
      "|WHITE METAL LANTERN               |[WHITE, METAL, LANTERN]                 |WHITE   |\n",
      "|WHITE METAL LANTERN               |[WHITE, METAL, LANTERN]                 |METAL   |\n",
      "|WHITE METAL LANTERN               |[WHITE, METAL, LANTERN]                 |LANTERN |\n",
      "+----------------------------------+----------------------------------------+--------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando view\n",
    "df.select(\"Description\").createOrReplaceTempView(\"vw_description\")\n",
    "\n",
    "# Explodindo array em SQL com LATERAL VIEW\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        Description,\n",
    "        split(Description, \" \") AS splitted,\n",
    "        exploded\n",
    "    \n",
    "    FROM vw_description\n",
    "    \n",
    "    LATERAL VIEW\n",
    "        explode(split(Description, \" \")) t AS exploded\n",
    "\"\"\").show(8, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8edca8",
   "metadata": {},
   "source": [
    "## Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d1e14c55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------+---------+-----------------------------------------------+\n",
      "|Description                        |InvoiceNo|complex_map                                    |\n",
      "+-----------------------------------+---------+-----------------------------------------------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER |536365   |{WHITE HANGING HEART T-LIGHT HOLDER -> 536365} |\n",
      "|WHITE METAL LANTERN                |536365   |{WHITE METAL LANTERN -> 536365}                |\n",
      "|CREAM CUPID HEARTS COAT HANGER     |536365   |{CREAM CUPID HEARTS COAT HANGER -> 536365}     |\n",
      "|KNITTED UNION FLAG HOT WATER BOTTLE|536365   |{KNITTED UNION FLAG HOT WATER BOTTLE -> 536365}|\n",
      "|RED WOOLLY HOTTIE WHITE HEART.     |536365   |{RED WOOLLY HOTTIE WHITE HEART. -> 536365}     |\n",
      "+-----------------------------------+---------+-----------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import create_map\n",
    "\n",
    "# Criando um campo com map\n",
    "df_map = df.select(\n",
    "    \"Description\",\n",
    "    \"InvoiceNo\",\n",
    "    create_map(\"Description\", \"InvoiceNo\").alias(\"complex_map\")\n",
    ")\n",
    "df_map.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1777247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|complex_map[WHITE METAL LANTERN]|\n",
      "+--------------------------------+\n",
      "|                            null|\n",
      "|                          536365|\n",
      "|                            null|\n",
      "+--------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando valores em um campo map\n",
    "df_map.selectExpr(\"complex_map['WHITE METAL LANTERN']\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "148731ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+---------+----------------------------------+------+\n",
      "|Description                       |InvoiceNo|key                               |value |\n",
      "+----------------------------------+---------+----------------------------------+------+\n",
      "|WHITE HANGING HEART T-LIGHT HOLDER|536365   |WHITE HANGING HEART T-LIGHT HOLDER|536365|\n",
      "|WHITE METAL LANTERN               |536365   |WHITE METAL LANTERN               |536365|\n",
      "|CREAM CUPID HEARTS COAT HANGER    |536365   |CREAM CUPID HEARTS COAT HANGER    |536365|\n",
      "+----------------------------------+---------+----------------------------------+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 'Explodindo' valores em um campo map\n",
    "df_map.select(\"Description\", \"InvoiceNo\", explode(\"complex_map\")).show(3, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "86e38763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+-----------------------------------+-------+\n",
      "|complex_map                                    |desc                               |invoice|\n",
      "+-----------------------------------------------+-----------------------------------+-------+\n",
      "|{WHITE HANGING HEART T-LIGHT HOLDER -> 536365} |WHITE HANGING HEART T-LIGHT HOLDER |536365 |\n",
      "|{WHITE METAL LANTERN -> 536365}                |WHITE METAL LANTERN                |536365 |\n",
      "|{CREAM CUPID HEARTS COAT HANGER -> 536365}     |CREAM CUPID HEARTS COAT HANGER     |536365 |\n",
      "|{KNITTED UNION FLAG HOT WATER BOTTLE -> 536365}|KNITTED UNION FLAG HOT WATER BOTTLE|536365 |\n",
      "|{RED WOOLLY HOTTIE WHITE HEART. -> 536365}     |RED WOOLLY HOTTIE WHITE HEART.     |536365 |\n",
      "+-----------------------------------------------+-----------------------------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando view\n",
    "df.select(\"Description\", \"InvoiceNo\").createOrReplaceTempView(\"vw_desc_invoice\")\n",
    "\n",
    "# Em SQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        map(Description, InvoiceNo) AS complex_map,\n",
    "        explode(map(Description, InvoiceNo)) AS (desc, invoice)\n",
    "        \n",
    "    FROM vw_desc_invoice\n",
    "    \n",
    "\"\"\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51ee51d",
   "metadata": {},
   "source": [
    "# Trabalhando com JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3105ab65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|jsonString                               |\n",
      "+-----------------------------------------+\n",
      "|{\"myJSONKey\": {\"myJSONValue\": [1, 2, 3]}}|\n",
      "+-----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando coluna JSON\n",
    "df_json = spark.range(1)\\\n",
    "    .selectExpr(\"\"\"\n",
    "        '{\"myJSONKey\": {\"myJSONValue\": [1, 2, 3]}}' as jsonString\n",
    "    \"\"\")\n",
    "df_json.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "25ef8f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------------------+\n",
      "|column|c0                     |\n",
      "+------+-----------------------+\n",
      "|2     |{\"myJSONValue\":[1,2,3]}|\n",
      "+------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import get_json_object, json_tuple\n",
    "\n",
    "# Retornando valores JSON\n",
    "df_json.select(\n",
    "    get_json_object(col(\"jsonString\"), \"$.myJSONKey.myJSONValue[1]\").alias(\"column\"),\n",
    "    json_tuple(col(\"jsonString\"), \"myJSONKey\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "87867aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+\n",
      "|to_json(myStruct)                                                         |\n",
      "+--------------------------------------------------------------------------+\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE HANGING HEART T-LIGHT HOLDER\"} |\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"WHITE METAL LANTERN\"}                |\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"CREAM CUPID HEARTS COAT HANGER\"}     |\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"KNITTED UNION FLAG HOT WATER BOTTLE\"}|\n",
      "|{\"InvoiceNo\":\"536365\",\"Description\":\"RED WOOLLY HOTTIE WHITE HEART.\"}     |\n",
      "+--------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformando StructType em JSON\n",
    "from pyspark.sql.functions import to_json\n",
    "\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    "    .select(to_json(col('myStruct')))\\\n",
    "    .show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "87511363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|             newJSON|  from_json(newJSON)|\n",
      "+--------------------+--------------------+\n",
      "|{\"InvoiceNo\":\"536...|{536365, WHITE HA...|\n",
      "|{\"InvoiceNo\":\"536...|{536365, WHITE ME...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql.functions import from_json\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Definindo schema\n",
    "parseSchema = StructType((\n",
    "    StructField(\"InvoiceNo\", StringType(), True),\n",
    "    StructField(\"Description\", StringType(), True)\n",
    "))\n",
    "\n",
    "# Criando consulta para conversão de json em Struct\n",
    "df.selectExpr(\"(InvoiceNo, Description) as myStruct\")\\\n",
    "    .select(to_json(col(\"myStruct\")).alias(\"newJSON\"))\\\n",
    "    .select(col(\"newJSON\"), from_json(col(\"newJSON\"), parseSchema))\\\n",
    "    .show(2, truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1ec055d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Definindo função\n",
    "def power3(value):\n",
    "    return value ** 3\n",
    "\n",
    "power3(2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "8df50030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "|num|power3(num)|\n",
      "+---+-----------+\n",
      "|  0|          0|\n",
      "|  1|          1|\n",
      "|  2|          8|\n",
      "|  3|         27|\n",
      "|  4|         64|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "# Registrando udf\n",
    "power3udf = udf(power3)\n",
    "\n",
    "# Executando udf\n",
    "df_udf = spark.range(5).toDF(\"num\")\n",
    "df_udf.select(\"num\", power3udf(col(\"num\"))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "bd2d1e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "+-----------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-----------+\n",
      "|power3(num)|\n",
      "+-----------+\n",
      "|          0|\n",
      "|          1|\n",
      "|          8|\n",
      "+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registrando função para uso em SQL\n",
    "spark.udf.register(\"power3\", power3)\n",
    "\n",
    "# Executando função como uma expressão\n",
    "df_udf.selectExpr(\"power3(num)\").show(3)\n",
    "\n",
    "# Executando função com SparkSQL\n",
    "df_udf.createOrReplaceTempView(\"df_udf\")\n",
    "spark.sql(\"\"\"SELECT power3(num) FROM df_udf\"\"\").show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "72c1707c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "|num|power3_double(num)|\n",
      "+---+------------------+\n",
      "|  0|              null|\n",
      "|  1|              null|\n",
      "|  2|              null|\n",
      "|  3|              null|\n",
      "|  4|              null|\n",
      "+---+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registrando função com retorno explícito\n",
    "spark.udf.register(\"power3_double\", power3, DoubleType())\n",
    "\n",
    "# Executando função em inteiros\n",
    "df_udf.selectExpr(\"num\", \"power3_double(num)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "e549a42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------------+\n",
      "|num|power3_final(num)|\n",
      "+---+-----------------+\n",
      "|  0|              0.0|\n",
      "|  1|              1.0|\n",
      "|  2|              8.0|\n",
      "|  3|             27.0|\n",
      "|  4|             64.0|\n",
      "+---+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Registrando função com casting explícito\n",
    "spark.udf.register(\"power3_final\", lambda x: float(x ** 3), DoubleType())\n",
    "\n",
    "# Executando função em inteiros\n",
    "df_udf.selectExpr(\"num\", \"power3_final(num)\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42bf50c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

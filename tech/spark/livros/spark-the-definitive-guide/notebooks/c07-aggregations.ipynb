{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f23ea4d",
   "metadata": {},
   "source": [
    "**_Objetivo:_** Neste notebook, serão consolidados códigos para explorações práticas envolvendo o contéudo presente no capítulo 7 do livro Spark - The Definitive Guide: Aggregations. No cenário proposto, exemplos de operações de agregação e sumarização de dados são fornecidos de modo a propor um entendimento claro sobre como tais transformações podem ser úteis no dia a dia de construção de fluxos de ETL e extração de insights de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dacbc035",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-CJKTBO0:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.2.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b9543bfbb0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importando bibliotecas\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Inicializando sessão spark\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea437e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definindo variáveis de diretório\n",
    "DATA_PATH = '../book-github-resources/Spark-The-Definitive-Guide-master/data/retail-data/all/online-retail-dataset.csv'\n",
    "\n",
    "# Lendo base de dados\n",
    "df = spark.read\\\n",
    "    .format('csv')\\\n",
    "    .option('inferSchema', 'true')\\\n",
    "    .option('header', 'true')\\\n",
    "    .load(DATA_PATH)\n",
    "\n",
    "# Aplicando cache e criando view\n",
    "df.cache()\n",
    "df.createOrReplaceTempView('vw_retail')\n",
    "\n",
    "# Visualizando amostra\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fcc4c077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo de agragação simples\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d0413c",
   "metadata": {},
   "source": [
    "# Funções de Agregação"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78b8051",
   "metadata": {},
   "source": [
    "## count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be1de811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ação: 541909\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n",
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|  541909|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "# count como ação\n",
    "print(f'Ação: {df.count()}\\n')\n",
    "\n",
    "# count como transformação\n",
    "df.select(count(\"*\")).show()\n",
    "\n",
    "# count via SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT count(1) FROM vw_retail\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532c3e90",
   "metadata": {},
   "source": [
    "## countDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33b71193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n",
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import countDistinct\n",
    "\n",
    "# Contando valores distintos\n",
    "df.select(countDistinct(\"StockCode\")).show()\n",
    "\n",
    "# utilizando SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COUNT(DISTINCT StockCode) FROM vw_retail\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb9542",
   "metadata": {},
   "source": [
    "## aprox_count_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dc9e3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n",
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3364|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "# Contando valores distintos com taxa de erro permitida\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show()\n",
    "\n",
    "# Via SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT approx_count_distinct(StockCode, 0.1) FROM vw_retail\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d20d13",
   "metadata": {},
   "source": [
    "## first e last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b1b6798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "| first| last|\n",
      "+------+-----+\n",
      "|85123A|22138|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "| first| last|\n",
      "+------+-----+\n",
      "|85123A|22138|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "# Retornando valores\n",
    "df.select(\n",
    "    first(\"StockCode\").alias(\"first\"),\n",
    "    last(\"StockCode\").alias(\"last\")\n",
    ").show()\n",
    "\n",
    "# Via SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        first(StockCode) AS first,\n",
    "        last(StockCode) AS last\n",
    "    FROM vw_retail\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad8a08be",
   "metadata": {},
   "source": [
    "## min e max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "380b6911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   min|  max|\n",
      "+------+-----+\n",
      "|-80995|80995|\n",
      "+------+-----+\n",
      "\n",
      "+------+-----+\n",
      "|   min|  max|\n",
      "+------+-----+\n",
      "|-80995|80995|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "# Executando consulta\n",
    "df.select(\n",
    "    min(\"Quantity\").alias(\"min\"),\n",
    "    max(\"Quantity\").alias(\"max\")\n",
    ").show()\n",
    "\n",
    "# Via SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        min(Quantity) AS min,\n",
    "        max(Quantity) AS max\n",
    "    FROM vw_retail\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acae48d",
   "metadata": {},
   "source": [
    "## sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "faf9fb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n",
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "# Realizando consulta\n",
    "df.select(sum(\"Quantity\")).show()\n",
    "\n",
    "# Via SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT sum(Quantity) FROM vw_retail\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7589bce1",
   "metadata": {},
   "source": [
    "## sumDistinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be5b8f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n",
      "+----------------------+\n",
      "|sum(DISTINCT Quantity)|\n",
      "+----------------------+\n",
      "|                 29310|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando função\n",
    "from pyspark.sql.functions import sum_distinct\n",
    "\n",
    "# Realizando consulta\n",
    "df.select(sum_distinct(\"Quantity\")).show()\n",
    "\n",
    "# Via SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT sum(DISTINCT Quantity) FROM vw_retail\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de40e904",
   "metadata": {},
   "source": [
    "## avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49f3c251",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+\n",
      "|       sum/count|             avg|            mean|\n",
      "+----------------+----------------+----------------+\n",
      "|9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+----------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "# Realizando seleção\n",
    "df.select(\n",
    "    (sum(\"Quantity\") / count(\"Quantity\")).alias(\"sum/count\"),\n",
    "    avg(\"Quantity\").alias(\"avg\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78392855",
   "metadata": {},
   "source": [
    "## var e stddev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "14eb381f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+-----------------+------------------+\n",
      "|         var_samp|       stddev_samp|          var_pop|        stddev_pop|\n",
      "+-----------------+------------------+-----------------+------------------+\n",
      "|47559.39140929892|218.08115785023455|47559.30364660923|218.08095663447835|\n",
      "+-----------------+------------------+-----------------+------------------+\n",
      "\n",
      "+-----------------+------------------+\n",
      "|         variance|            stddev|\n",
      "+-----------------+------------------+\n",
      "|47559.39140929892|218.08115785023455|\n",
      "+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import variance, stddev, \\\n",
    "                                  var_pop, stddev_pop, \\\n",
    "                                  var_samp, stddev_samp\n",
    "\n",
    "# Realizando consultas\n",
    "df.select(\n",
    "    var_samp(\"Quantity\").alias(\"var_samp\"),\n",
    "        stddev_samp(\"Quantity\").alias(\"stddev_samp\"),\n",
    "    var_pop(\"Quantity\").alias(\"var_pop\"),\n",
    "    stddev_pop(\"Quantity\").alias(\"stddev_pop\")\n",
    ").show()\n",
    "\n",
    "# Sem especificação do público\n",
    "df.select(\n",
    "    variance(\"Quantity\").alias(\"variance\"),\n",
    "    stddev(\"Quantity\").alias(\"stddev\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02829443",
   "metadata": {},
   "source": [
    "## skewness e kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6135f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|  skewness(Quantity)|kurtosis(Quantity)|\n",
      "+--------------------+------------------+\n",
      "|-0.26407557610528376|119768.05495530753|\n",
      "+--------------------+------------------+\n",
      "\n",
      "+--------------------+------------------+\n",
      "|  skewness(Quantity)|kurtosis(Quantity)|\n",
      "+--------------------+------------------+\n",
      "|-0.26407557610528376|119768.05495530753|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "\n",
    "# Definindo consulta\n",
    "df.select(\n",
    "    skewness(\"Quantity\"),\n",
    "    kurtosis(\"Quantity\")\n",
    ").show()\n",
    "\n",
    "# Via SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        skewness(Quantity),\n",
    "        kurtosis(Quantity)\n",
    "    FROM vw_retail\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d02dc84",
   "metadata": {},
   "source": [
    "## corr e covar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d7dd9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-------------------+\n",
      "|                corr|          covar_pop|         covar_samp|\n",
      "+--------------------+-------------------+-------------------+\n",
      "|-0.00123492454487...|-26.058713170968105|-26.058761257937057|\n",
      "+--------------------+-------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "\n",
    "# Definindo consulta\n",
    "df.select(\n",
    "    corr(\"Quantity\", \"UnitPrice\").alias(\"corr\"),\n",
    "    covar_pop(\"Quantity\", \"UnitPrice\").alias(\"covar_pop\"),\n",
    "    covar_samp(\"Quantity\", \"UnitPrice\").alias(\"covar_samp\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024adc38",
   "metadata": {},
   "source": [
    "## Agregando Tipos Complexos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98076c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n",
      "|collect_set(Country)|collect_list(Country)|\n",
      "+--------------------+---------------------+\n",
      "|[Portugal, Italy,...| [United Kingdom, ...|\n",
      "+--------------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import collect_set, collect_list, size\n",
    "\n",
    "# Definido consultas\n",
    "df.select(\n",
    "    collect_set(\"Country\"),\n",
    "    collect_list(\"Country\")\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e551716b",
   "metadata": {},
   "source": [
    "# Agrupamentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99dc14ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---------+-------+\n",
      "|InvoiceNo|CustomerId|count_qty|sum_qty|\n",
      "+---------+----------+---------+-------+\n",
      "|   536846|     14573|       76|    134|\n",
      "|   537026|     12395|       12|    528|\n",
      "|   537883|     14437|        5|     60|\n",
      "|   538068|     17978|       12|    499|\n",
      "|   538279|     14952|        7|    472|\n",
      "+---------+----------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funçõpes\n",
    "from pyspark.sql.functions import count, sum, avg\n",
    "\n",
    "# Definindo consulta com groupBy\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").agg(\n",
    "    count(\"Quantity\").alias(\"count_qty\"),\n",
    "    sum(\"Quantity\").alias(\"sum_qty\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a383485b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-------+------+\n",
      "|InvoiceNo|CustomerId|avg_qty|stddev|\n",
      "+---------+----------+-------+------+\n",
      "|   536846|     14573|   1.76|  1.61|\n",
      "|   537026|     12395|   44.0| 48.29|\n",
      "|   537883|     14437|   12.0|   0.0|\n",
      "|   538068|     17978|  41.58|123.03|\n",
      "|   538279|     14952|  67.43|  76.8|\n",
      "+---------+----------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agrupando com expressões\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").agg(\n",
    "    expr(\"round(avg(Quantity), 2) AS avg_qty\"),\n",
    "    expr(\"round(stddev(Quantity), 2) AS stddev\")\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1db37be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+-------+-------+------+\n",
      "|InvoiceNo|CustomerId|count|sum_qty|avg_qty|stddev|\n",
      "+---------+----------+-----+-------+-------+------+\n",
      "|   536846|     14573|   76|    134|   1.76|  1.61|\n",
      "|   537026|     12395|   12|    528|   44.0| 48.29|\n",
      "|   537883|     14437|    5|     60|   12.0|   0.0|\n",
      "|   538068|     17978|   12|    499|  41.58|123.03|\n",
      "|   538279|     14952|    7|    472|  67.43|  76.8|\n",
      "+---------+----------+-----+-------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Realizando os mesmos cálculos via SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        InvoiceNo,\n",
    "        CustomerId,\n",
    "        count(Quantity) AS count,\n",
    "        sum(Quantity) AS sum_qty,\n",
    "        round(avg(Quantity), 2) AS avg_qty,\n",
    "        round(stddev(Quantity), 2) AS stddev\n",
    "    \n",
    "    FROM vw_retail\n",
    "    \n",
    "    GROUP BY\n",
    "        InvoiceNo,\n",
    "        CustomerId\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d14d196d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "947d04bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+\n",
      "|   InvoiceDate|      date|\n",
      "+--------------+----------+\n",
      "|12/1/2010 8:26|2010-12-01|\n",
      "|12/1/2010 8:26|2010-12-01|\n",
      "|12/1/2010 8:26|2010-12-01|\n",
      "|12/1/2010 8:26|2010-12-01|\n",
      "|12/1/2010 8:26|2010-12-01|\n",
      "+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "# Transformando coluna de data\n",
    "df_date = df.withColumn(\n",
    "    \"date\", to_date(\"InvoiceDate\", \"M/d/yyyy H:mm\")\n",
    ")\n",
    "df_date.createOrReplaceTempView(\"vw_retail_date\")\n",
    "\n",
    "# Verificando dados\n",
    "df_date.select(\"InvoiceDate\", \"date\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "727392ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importando funções\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import desc, max, rank, dense_rank\n",
    "\n",
    "# Especificando janela de análise\n",
    "windowSpec = Window\\\n",
    "    .partitionBy(\"CustomerId\", \"date\")\\\n",
    "    .orderBy(desc(\"Quantity\"))\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9b158e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificando agregação\n",
    "maxPurchaseQty = max(col(\"Quantity\")).over(windowSpec)\n",
    "\n",
    "# Criando rankings\n",
    "purchaseRank = rank().over(windowSpec)\n",
    "purchaseDenseRank = dense_rank().over(windowSpec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4dde195d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+-------+------------+--------------+\n",
      "|CustomerId|      date|Quantity|qtyRank|qtyDenseRank|maxPurchaseQty|\n",
      "+----------+----------+--------+-------+------------+--------------+\n",
      "|     12346|2011-01-18|   74215|      1|           1|         74215|\n",
      "|     12346|2011-01-18|  -74215|      2|           2|         74215|\n",
      "|     12347|2010-12-07|      36|      1|           1|            36|\n",
      "|     12347|2010-12-07|      30|      2|           2|            36|\n",
      "|     12347|2010-12-07|      24|      3|           3|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|      12|      4|           4|            36|\n",
      "|     12347|2010-12-07|       6|     17|           5|            36|\n",
      "|     12347|2010-12-07|       6|     17|           5|            36|\n",
      "+----------+----------+--------+-------+------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Selecionando dados\n",
    "df_date.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n",
    "    .select(\n",
    "        \"CustomerId\",\n",
    "        \"date\",\n",
    "        \"Quantity\",\n",
    "        purchaseRank.alias(\"qtyRank\"),\n",
    "        purchaseDenseRank.alias(\"qtyDenseRank\"),\n",
    "        maxPurchaseQty.alias(\"maxPurchaseQty\")\n",
    "    ).show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "653fc55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+--------+----+-----+-----------+\n",
      "|CustomerId|      date|Quantity|rank|dRank|maxPurchase|\n",
      "+----------+----------+--------+----+-----+-----------+\n",
      "|     12346|2011-01-18|   74215|   1|    1|      74215|\n",
      "|     12346|2011-01-18|  -74215|   2|    2|      74215|\n",
      "|     12347|2010-12-07|      36|   1|    1|         36|\n",
      "|     12347|2010-12-07|      30|   2|    2|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      24|   3|    3|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|      12|   4|    4|         36|\n",
      "|     12347|2010-12-07|       6|  17|    5|         36|\n",
      "|     12347|2010-12-07|       6|  17|    5|         36|\n",
      "+----------+----------+--------+----+-----+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplicando a mesma análise via SparkSQL\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CustomerId,\n",
    "        date,\n",
    "        Quantity,\n",
    "        \n",
    "        rank(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                             ORDER BY Quantity DESC NULLS LAST\n",
    "                             ROWS BETWEEN\n",
    "                                 UNBOUNDED PRECEDING AND\n",
    "                                 CURRENT ROW) AS rank,\n",
    "                                 \n",
    "        dense_rank(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                                   ORDER BY Quantity DESC NULLS LAST\n",
    "                                    ROWS BETWEEN\n",
    "                                        UNBOUNDED PRECEDING AND\n",
    "                                        CURRENT ROW) AS dRank,\n",
    "        \n",
    "        max(Quantity) OVER (PARTITION BY CustomerId, date\n",
    "                            ORDER BY Quantity DESC NULLS LAST\n",
    "                            ROWS BETWEEN\n",
    "                                UNBOUNDED PRECEDING AND\n",
    "                                CURRENT ROW) AS maxPurchase\n",
    "                                \n",
    "    FROM vw_retail_date WHERE CustomerId IS NOT NULL ORDER BY CustomerId\n",
    "\n",
    "\"\"\").show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4a60d9",
   "metadata": {},
   "source": [
    "# Grouping Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "1bf45a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|StockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|     18287|    85173|           48|\n",
      "|     18287|   85040A|           48|\n",
      "|     18287|   85039B|          120|\n",
      "|     18287|   85039A|           96|\n",
      "|     18287|    84920|            4|\n",
      "+----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dropando nulos e criando nova view\n",
    "df_not_null = df_date.na.drop()\n",
    "df_not_null.createOrReplaceTempView(\"vw_retail_not_null\")\n",
    "\n",
    "# Aplicando grouping set\n",
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CustomerId,\n",
    "        StockCode,\n",
    "        sum(Quantity)\n",
    "        \n",
    "    FROM vw_retail_not_null\n",
    "    \n",
    "    GROUP BY CustomerId, StockCode\n",
    "    \n",
    "    GROUPING SETS((CustomerId, StockCode))\n",
    "    \n",
    "    ORDER BY CustomerId DESC, StockCode DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "43f86bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+\n",
      "|CustomerId|StockCode|sum(Quantity)|\n",
      "+----------+---------+-------------+\n",
      "|      null|     null|      4906888|\n",
      "|     14646|     null|       196719|\n",
      "|     12415|     null|        77242|\n",
      "|     14911|     null|        77180|\n",
      "|     17450|     null|        69029|\n",
      "+----------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT\n",
    "        CustomerId,\n",
    "        StockCode,\n",
    "        sum(Quantity)\n",
    "        \n",
    "    FROM vw_retail_not_null\n",
    "    \n",
    "    GROUP BY CustomerId, StockCode\n",
    "    \n",
    "    GROUPING SETS((CustomerId, StockCode), (CustomerId), ())\n",
    "    \n",
    "    ORDER BY sum(Quantity) DESC, CustomerId DESC, StockCode DESC\n",
    "\"\"\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e15a9",
   "metadata": {},
   "source": [
    "## Rollups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "230fabbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n",
      "|      Date|       Country|total_quantity|\n",
      "+----------+--------------+--------------+\n",
      "|      null|          null|       4906888|\n",
      "|2010-12-01|United Kingdom|         21167|\n",
      "|2010-12-01|          null|         24032|\n",
      "|2010-12-01|     Australia|           107|\n",
      "|2010-12-01|        France|           449|\n",
      "+----------+--------------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rollup = df_not_null.rollup(\"date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    "    .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` AS total_quantity\")\\\n",
    "    .orderBy(\"Date\")\n",
    "df_rollup.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d58b4e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+-------------+\n",
      "|      date|       Country|sum(Quantity)|\n",
      "+----------+--------------+-------------+\n",
      "|      null|          null|      5176450|\n",
      "|2010-12-01|United Kingdom|        23949|\n",
      "|2010-12-01|          null|        26814|\n",
      "|2010-12-01|     Australia|          107|\n",
      "|2010-12-01|        France|          449|\n",
      "+----------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplicando rollup\n",
    "df_date.rollup(\"date\", \"Country\")\\\n",
    "    .agg(sum(\"Quantity\"))\\\n",
    "    .orderBy(\"Date\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d2ec99c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|sum(Quantity)|\n",
      "+-------------+\n",
      "|      5176450|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date.select(sum(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa3367e",
   "metadata": {},
   "source": [
    "## Cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "46c7fd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n",
      "|Date|             Country|sum(Quantity)|\n",
      "+----+--------------------+-------------+\n",
      "|null|United Arab Emirates|          982|\n",
      "|null|               Italy|         7999|\n",
      "|null|           Singapore|         5234|\n",
      "|null|             Finland|        10666|\n",
      "|null|              Greece|         1556|\n",
      "+----+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Aplicando agregações multidimensionais\n",
    "df_date.cube(\"Date\", \"Country\")\\\n",
    "    .agg(sum(\"Quantity\"))\\\n",
    "    .orderBy(\"Date\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "4c36518e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+-------------+\n",
      "|      Date|Country|sum(Quantity)|\n",
      "+----------+-------+-------------+\n",
      "|2011-01-11|   null|        29093|\n",
      "|2011-01-13|   null|        10114|\n",
      "|2010-12-19|   null|         3795|\n",
      "|2011-01-06|   null|        22461|\n",
      "|2011-02-10|   null|        11447|\n",
      "+----------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_date.cube(\"Date\", \"Country\")\\\n",
    "    .agg(sum(\"Quantity\"))\\\n",
    "    .orderBy(\"Country\")\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3db40fe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n",
      "|Date|             Country|sum(Quantity)|\n",
      "+----+--------------------+-------------+\n",
      "|null|             Finland|        10666|\n",
      "|null|           Lithuania|          652|\n",
      "|null|              Poland|         3653|\n",
      "|null|             Iceland|         2458|\n",
      "|null|United Arab Emirates|          982|\n",
      "+----+--------------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----------+-------+-------------+\n",
      "|      Date|Country|sum(Quantity)|\n",
      "+----------+-------+-------------+\n",
      "|2010-12-17|   null|        16069|\n",
      "|2010-12-14|   null|        20098|\n",
      "|2011-02-13|   null|         2715|\n",
      "|2011-01-06|   null|        22461|\n",
      "|2011-02-17|   null|        14544|\n",
      "+----------+-------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+----+-------+-------------+\n",
      "|Date|Country|sum(Quantity)|\n",
      "+----+-------+-------------+\n",
      "|null|   null|      5176450|\n",
      "+----+-------+-------------+\n",
      "\n",
      "+----------+--------------+-------------+\n",
      "|      Date|       Country|sum(Quantity)|\n",
      "+----------+--------------+-------------+\n",
      "|2010-12-17|       Germany|         1657|\n",
      "|2011-04-01|        France|          327|\n",
      "|2011-04-07|          EIRE|          390|\n",
      "|2011-01-12|United Kingdom|         8622|\n",
      "|2011-01-19|         Spain|           48|\n",
      "+----------+--------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando cubo\n",
    "df_cube = df_date.cube(\"Date\", \"Country\")\\\n",
    "    .agg(sum(\"Quantity\"))\n",
    "\n",
    "# Totalização por país\n",
    "df_cube.where(\"date is null\").show(5)\n",
    "\n",
    "# Totalização por data\n",
    "df_cube.where(\"country is null\").show(5)\n",
    "\n",
    "# Totalização independente de data e país\n",
    "df_cube.where(\"date is null AND country is null\").show()\n",
    "\n",
    "# Totalização por data e país\n",
    "df_cube.where(\"date is not null AND country is not null\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400a04ed",
   "metadata": {},
   "source": [
    "## Pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "71642449",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "e8e7ba19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----+----+----+-----+----+----+----+-----+-----+-----+-----+\n",
      "|  Country|   1|   2|   3|   4|    5|   6|   7|   8|    9|   10|   11|   12|\n",
      "+---------+----+----+----+----+-----+----+----+----+-----+-----+-----+-----+\n",
      "|   Sweden|3096| 250|5263| 310| 2829| 404|6006|1308| 4344| 6151| 1962| 3714|\n",
      "|Singapore|1091|null|null|1384| null|null|2160|null| null|  599| null| null|\n",
      "|  Germany|8906|4083|7675|5692|12951|7348|8991|9560|11028|17636|12922|10656|\n",
      "|      RSA|null|null|null|null| null|null|null|null| null|  352| null| null|\n",
      "|   France|9155|5301|8639|2216| 9780|9441|5656|7948|12912|13737|17086| 8609|\n",
      "+---------+----+----+----+----+-----+----+----+----+-----+-----+-----+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Criando coluna de mês e pivotando dados\n",
    "df_date.withColumn(\"month_dt\", month(\"date\"))\\\n",
    "    .groupBy(\"Country\")\\\n",
    "    .pivot(\"month_dt\")\\\n",
    "    .agg(sum(\"Quantity\"))\\\n",
    "    .show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ddb0e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
